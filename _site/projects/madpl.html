<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <script src="//assets/katex/katex.min.js"></script>
  <script src="//assets/pseudocode/pseudocode.js" type="text/javascript"></script>
  <link rel="stylesheet" href="//assets/katex/katex.min.css">
  <link rel="stylesheet" href="//assets/pseudocode/pseudocode.css" type="text/css">

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Model-based Offline Multi-Agent Dialogue Policy Learning &middot; Anubhav Sachan
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">

  <link rel="stylesheet" href="/public/css/font-awesome.min.css">
  <link rel="stylesheet" href="/public/css/font-awesome.css">

 <!--  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->
 <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;700&display=swap" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0c">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <p style="font-size:46px; color: #fff;">
          Anubhav <b>Sachan</b>
      </p>
      <p style="font-size:20px;">
          Learning with<br>experience replay! <i class="fa fa-star"></i>
      </p>
      <p></p>
    </div>
    <hr width="90">
    <nav class="sidebar-nav" style="font-size: 90%">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/">Blog</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/experience/">Experience</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/projects/">Projects</a>
          
        
      

<!--       <a class="sidebar-nav-item" href="https://github.com/anubhav4sachan//archive/v2.1.0.zip">Download</a>
      <a class="sidebar-nav-item" href="https://github.com/anubhav4sachan/">GitHub project</a>
      <span class="sidebar-nav-item">Currently v2.1.0</span> -->
      <a class="sidebar-nav-item" href="http://localhost:4000/assets/Anubhav_Sachan_CV.pdf">Resume</a>
    </nav>

<a href="https://www.github.com/anubhav4sachan/" target="_blank"><i class="fa fa-github"></i></a> &middot; <a href="https://www.linkedin.com/in/anubhav4sachan/" target="_blank"><i class="fa fa-linkedin-square"></i></a> &middot; <a href="https://www.twitter.com/anubhav4sachan/" target="_blank"><i class="fa fa-twitter"></i></a> &middot; <a href="https://www.medium.com/@anubhav4sachan/" target="_blank"><i class="fa fa-medium"></i></a> &middot; <a href="https://instagram.com/anubhavenue/" target="_blank"><i class="fa fa-instagram"></i></a> &middot; <a href="mailto:hi@anubhavsachan.com" target="_blank"><i class="fa fa-envelope-o"></i></a>
<br><br>
    <p style="font-size: 50%">&copy; 2020 | Built with <i class="fa fa-heart"></i> by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> &amp; <a href="https://github.com/poole/hyde/" target="_blank">Hyde</a>.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Model-based Offline Multi-Agent Dialogue Policy Learning</h1>
  <span class="post-date">June, 2020</span>
  <p>The conventional problem of selecting an action when provided with a state, environment and history (context) falls in the domain of self-play reinforcement learning. Numerous such algorithms learn a dialog policy with the reward function requiring an elaborate design of a comprehensive user simulator and pre-specified user goals. Here, in the implemented method, it is regarded that the user agent can learn with the system agent in a joint/shared fashion. The method involves the concept of role-aware reward decomposition using Hybrid Value Network with the integration of actor-critic framework to maximize the global reward for the policy learner.</p>

<h2 id="introduction">Introduction</h2>
<p>Dialog Policy plays a vital role in deciding the subsequent action for a conversation and hence, responsible for steering the dialog to the desired goal. The task oriented dialog systems, usually, aim at <em>concise</em> conversations.</p>

<p>The dialogue managers are frequently modeled as a sequential decision-making problem, where reinforcement learning (RL) is used for learning an optimal dialogue policy from user experiences. While a variety of RL methods have been developed for learning dialogue policies, the methods typically require a large amount of dialogue corpus until one can learn a good-quality dialogue policy. This method is quite infeasible, in the real world and shall be considered as a liability for a venture.</p>

<p>Offline learning focuses on learning the policies entirely from a batch of previously collected data. This problem setting is compelling, because it offers the promise of utilizing large, diverse, previously collected datasets to acquire policies without any costly or dangerous active exploration, but it is also exceptionally difficult, due to the distributional shift between the offline training data and the learned policy.</p>

<h2 id="multi-agent-dialogue-policy-learning">Multi-Agent Dialogue Policy Learning</h2>
<h3 id="the-architecture">The Architecture</h3>

<p>The learning method requires an annotated dialogue corpus \(\mathcal{D}\). Specifically, the user goal must be defined as \(\texttt{G = (C, R)}\) where \(\texttt{C}\) represents the constraints and \(\texttt{R}\) corresponds to the requests. There also exists an external database \(\texttt{DB}\), which can accessed to provide the required information and fulfil the requests. \(\texttt{G}\) can have multiple domains, and both the agents accomplish the sub-tasks in each of the domain. The agents are in a partially observable environment, hence, corresponds to Partially Observable Markov Decision Process. Precisely, the user agent has access to only \(\texttt{G}\) and the system agent has access to the \(\texttt{DB}\). The only way to gain information from each other is to engage in an interaction through dialogues.</p>

<center>
<figure>
<img src="/assets/images/architecture.png" alt="architecture" style="width: 80%;" />
<figcaption style="font-size: 10pt;">The basic architecture for Dialogue Policy Learning.</figcaption>
</figure>
</center>

<p>The designed architecture, for task-oriented dialogue systems, has reward decomposition for the user as well as the system agent during the conversation. The user and the system interact with each other as dialogue agents to jointly learn the policy. They can interact with each other, only and only through an abstract representation of an intention called a <em>dialogue act</em>. It is, in our case, represented in a quadruple of form <em>[domain, intent, slot-type, slot-value]</em>.</p>

<p>The algorithm is largely based on the Actor Critic Framework following the paradigm of centralized training and decentralized execution in multi agent reinforcement learning.  It basically asserts that the actor selects its action conditioned on local states, actions for a particular agent whereas the critic is based on all the agents. The Hybrid Value Network consists of three critics in which each critic estimates its return based on role-aware reward decomposition, and each actor uses the estimated value to optimize itself.</p>

<h3 id="dialogue-policies-and-reward-distribution">Dialogue Policies and Reward Distribution</h3>

<h4 id="dialogue-policies">Dialogue Policies:</h4>
<p><strong><em>System Policy:</em></strong> The system policy \(\pi\) decides the system action \(a^S\) according to the system dialog state \(s^S\) to give an appropriate response the user agent. Each system action is a subset of dialogue act set \(\mathcal{A}\) as there may be multiple intents a dialog turn. In practice, dialog acts are de-lexicalized in the dialog policy. The slot values are replaced with a count placeholder and is refilled with the true value according to the entity selected from the external database \(\texttt{DB}\). This faciliates the system to operate on unseen values.</p>

<p>The system dialog state \(s^S\) at dialog turn <em>t</em> is the concatenation of:</p>
<ul>
  <li>User action at current turn \(a^U_t\);</li>
  <li>System action \(a^S_{t-1}\) at the last turn;</li>
  <li>The belief state \(b_t\) that keeps track of constraint &amp; request slots given by user agent, and</li>
  <li>Embedding vectors of the number of query results \(q_t\) from \(\texttt{DB}\).</li>
</ul>

<p><strong><em>User Policy:</em></strong> The user policy \(\mu\) decides the user action \(a^U\) according to the user dialog state \(s^U\) to express the constraints (\(\texttt{C}\)) and requests (\(\texttt{R}\)) to the system agent.</p>

<p>The user dialog state \(s^U\) at dialog turn <em>t</em> is the concatenation of:</p>
<ul>
  <li>Last system action \(a^S_{t-1}\);</li>
  <li>Last user action \(a^U_{t-1}\);</li>
  <li>The goal state \(g_t\), and;</li>
  <li>An inconsistency vector \(c_t\) to indicate the inconsistency between system response and user constraint.</li>
</ul>

<p>In addition to the prediction of dialogue acts, user policy also predicts the terminal signal <em>T</em> at the same time, such that \(\mu = \mu (a^U, T \mid s^U)\).</p>

<h4 id="rewards-distribution">Rewards Distribution:</h4>
<p>The roles of both the agents, user as well as the system are quite different. The user agent actively initiates a task and may change it during conversation, but the system agent passively responds to the user agent and returns the proper information, so the reward should be considered separately for each agent. On the other hand, two agents communicate and collaborate to accomplish the same task cooperatively, so the reward also involves a global target for both agents. Hence, three fold reward distribution is considered as described.</p>

<ul>
  <li><strong><em>System Reward</em></strong> (\(r^S_t\)) consists of:
    <ul>
      <li>Empty dialog act penalty \(a^S_t = \varnothing\);</li>
      <li>Late answer penalty, if there is a request slot triggered but the system agent does not reply the information immediately, and;</li>
      <li>Task success reward based on the user agent’s description.</li>
    </ul>
  </li>
  <li><strong><em>User Reward</em></strong> (\(r^U_t\)) consists of:
    <ul>
      <li>Empty dialog act penalty \(a^U_t = \varnothing\);</li>
      <li>Early inform penalty, and;</li>
      <li>User goal reward, whether the user agents have expressed all the constraints \(\texttt{C}\) and requests \(\texttt{R}\).</li>
    </ul>
  </li>
  <li><strong><em>Global Reward</em></strong> (\(r^G_t\)) consists of:
    <ul>
      <li>Efficiency penalty, that a small negative value will be given at each dialog turn;</li>
      <li>Sub-goal completion reward once the sub-task of \(\texttt{G}\) in a particular domain is accomplished, and;</li>
      <li>Task success reward based on user goal \(\texttt{G}\).</li>
    </ul>
  </li>
</ul>

<p>Each agent obtains its <em>local reward</em> during each turn, and the <em>global reward</em> is awarded during the training of the model architecture.</p>

<h3 id="hybrid-value-network">Hybrid Value Network</h3>
<p>The aim of the value network is to estimate the expected return given a current state so that the policy can directly use the estimated cumulative reward for optimization, without sampling the trajectories to obtain rewards which may cause high variance. The actor critic approach in a multi agent reinforcement learning can easily be integrated with centralized training and decentralized execution framework i.e. actor of each agent benefits from a critic and the critic is augmented with the information from all the agents during the training phase.</p>

<p>The Hybrid Value Network (HVN) is inspired from Hybrid Reward Architecture for Reinforcement Learning. HVN improves the estimate of optimum role aware value function.</p>

<p>It first encodes the dialog state of each agent to learn a state representations:</p>
<center>
$$\begin{equation}
    h^S_s
   =
    tanh(f^S_s (s^S)),
\label{eq:hybrid_sys}
\end{equation}$$

$$\begin{equation}
    h^U_s
   =
    tanh(f^U_s (s^U))
\label{eq:hybrid_usr}
\end{equation}$$
</center>
<p>where \(f (.)\) can be any neural network unit. The value network is separated into branches each for the value of user reward (\(V^U(s^U)\)), system reward (\(V^S(s^S)\)) and global reward (\(V^G(s)\)).</p>

<h2 id="algorithm">Algorithm</h2>

<pre id="read-2" style="display:none;">
    
\begin{algorithm}
\caption{MADPL}
\begin{algorithmic}
\PROCEDURE{Pre-Training}{}
\STATE $\textbf{require:}$ Human to System Conversation Dialog Corpus $\mathcal{D}$ with the annotations of dialog acts $a$
    \STATE initialize: weights $\phi$, $\omega$ for system policy $\pi$ \&amp; user policy $\mu$ respectively
    \STATE pre-train: policies $\pi$, $\mu$ on $\mathcal{D}$ using equation 1.
\ENDPROCEDURE

\PROCEDURE{Training}{}
    \STATE initialize: the weights $\theta$ for the Hybrid Value Network ($V = V^S, V^U, V^G$) and target network $\theta^- \leftarrow \theta$
    \STATE $n = $ training iterations
    \FOR{$j = 0$ \TO $n$}
        \STATE initialize: user goal and dialog state $s^U$, $s^S$
        \STATE $\textbf{repeat:}$
        \STATE Sample actions $a^U$, $a^S$ and terminal signal $\mathcal{T}$ using current policy $\pi$, $\mu$
        \STATE Execute actions and observe reward $r^U$, $r^S$, $r^G$ and new states $s'^U$, $s'^S$
        \STATE Update hybrid value network (critic) using Eq. vhnet
        \STATE Compute the advantage $A^U$, $A^S$, $A^G$ using current value network
        \STATE Update two dialog policies (actor)
        \STATE Assign target network parameters $\theta^- \leftarrow \theta$ every C steps
        \STATE $\textbf{end repeat}$ at $\mathcal{T}$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}

</pre>
<div id="goal-2"></div>
<script type="text/javascript">
    var code = document.getElementById("read-2").textContent;
    var parentEl = document.getElementById("goal-2");
    var options = {
        lineNumber: true
    };
    pseudocode.render(code, parentEl, options);
</script>

<h3 id="pre-training-session">Pre-Training Session</h3>

<p>The initial two steps of the algorithm define the pre-training session. This involves the offline model-based training of the system policy ($\pi$) and the user policy ($\mu$) on human dialogue corpus $\mathcal{D}$ annotated with dialogue acts $\mathcal{a}$ according to the loss function as expressed in the equation 1.</p>
<center>
<figure>
    $$\begin{equation}
    L\hspace{2pt} (X, Y; \beta)
   =
    - [\beta \cdot Y^T \cdot log \hspace{2pt} \sigma (X)
    + (I - Y)^T \cdot log \hspace{2pt} (I - \sigma (X))]
\label{eq:pretrain_loss}
\end{equation}$$
<figcaption style="font-size: 10pt;">Equation 1</figcaption>
</figure>
</center>

<p>Here, X is the state and Y is the action from the corpus of the given task. The \(\beta\)-weighted logistic regression for policy pre-training here to alleviate data bias because each agent only generates several dialog acts in one dialog turn.</p>

<h3 id="training-session">Training Session</h3>

<p>The training of the algorithm is split into two stages, first, the pre-training stage and the second stage is the optimization process of both user as well as system policy.</p>

<p>For the critic optimization, it aims to minimize the squared error between the temporal difference target (\(r_t + \gamma V(s_{t+1})\)) and the estimated value (\(V(s_t) = \mathbb{E} \hspace{2pt}[r_t + \gamma V(s_{t+1})]\)). Due to the high variance in framework, the target network is introduced to bring stability in the training process. The actor-critic framework has high variance, since the critic part is updated too frequently and hence the changes done in estimated value bring instability in the training process. This is quite prominent in Multi Agent Reinforcement Learning.</p>

<p>In context of hybrid value network the session focuses on the minimization of loss functions described in equation 2.</p>

<center><figure>
$$
\begin{equation}
    L^S_V (\theta)
   =
    (r^S + \gamma V^S_{\theta^-} (s'^S) - V^S_\theta (s^S))^2,
\end{equation}$$
$$
\begin{equation}
    L^U_V (\theta)
   =
    (r^U + \gamma V^U_{\theta^-} (s'^U) - V^U_\theta (s^U))^2,
\end{equation}$$
$$
\begin{equation}
    L^G_V (\theta)
   =
    (r^G + \gamma V^G_{\theta^-} (s'^G) - V^G_\theta (s^G))^2,
\end{equation}$$
$$
\begin{equation}
    L_V
   =
    L^S_V + L^U_V + L^G_V
\label{hvnet}
\end{equation}$$
<figcaption style="font-size: 10pt;">Equation 2</figcaption>
</figure></center>

<p>\(V_\theta\) is parameterized by \(\theta\) and \(\theta^-\) is the weight of the target network. The overall loss is the sum of value estimation loss on each component reward. Each dialogue policy aims to maximize for the related returns i.e. system policy aims to maximize the cumulative system awards and global rewards, and similarly, the user policy.</p>

<p>The advantage (\(A(s) = r + \gamma V(s') - V(s)\)) estimated by the critic can evaluate the new state and the current state to determine the quality of the dialogue (better or worse). With the aid of HVN, the sum of the related component advantages can be used to update different agents.</p>

<h2 id="conclusion">Conclusion</h2>
<p>The new approach introduced in the paper Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition is quite insightful, and can be used for highly improved dialogue policy. The approach can be applied to leading datasets like DSTC8 - Schema Guided Dataset, to achieve state of the art results. There lies a great opportunity in improving the metrics viz. Inform F1 and the Match Rate, with the modification of the basic neural networks, to more complex networks, and using a better optimizer like AdamW. This project has been implemented using PyTorch Framework.</p>

</div>

    </div>

  </body>
</html>
