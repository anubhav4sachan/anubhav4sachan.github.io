<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Model-based Offline Multi-Agent Dialogue Policy Learning | Anubhav Sachan</title><meta name="generator" content="Jekyll v3.8.7" /><meta property="og:title" content="Model-based Offline Multi-Agent Dialogue Policy Learning" /><meta name="author" content="Anubhav Sachan" /><meta property="og:locale" content="en_US" /><meta name="description" content="The implemented learning paradigm relentlessly focuses on user agent to learn along with the system agent in a joint/shared fashion with the incorporation of the actor critic framework for the optimization of the model-based offline learned dialogue policy." /><meta property="og:description" content="The implemented learning paradigm relentlessly focuses on user agent to learn along with the system agent in a joint/shared fashion with the incorporation of the actor critic framework for the optimization of the model-based offline learned dialogue policy." /><link rel="canonical" href="http://localhost:4000/projects/madpl" /><meta property="og:url" content="http://localhost:4000/projects/madpl" /><meta property="og:site_name" content="Anubhav Sachan" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-06-15T00:00:00+05:30" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Model-based Offline Multi-Agent Dialogue Policy Learning" /><meta name="twitter:site" content="@anubhav4sachan" /><meta name="twitter:creator" content="@Anubhav Sachan" /> <script type="application/ld+json"> {"@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/madpl"},"url":"http://localhost:4000/projects/madpl","author":{"@type":"Person","name":"Anubhav Sachan"},"headline":"Model-based Offline Multi-Agent Dialogue Policy Learning","dateModified":"2020-06-15T00:00:00+05:30","description":"The implemented learning paradigm relentlessly focuses on user agent to learn along with the system agent in a joint/shared fashion with the incorporation of the actor critic framework for the optimization of the model-based offline learned dialogue policy.","datePublished":"2020-06-15T00:00:00+05:30","@context":"https://schema.org"}</script> <script src="/assets/katex/katex.min.js"></script> <script src="/assets/pseudocode/pseudocode.js" type="text/javascript"></script><link rel="stylesheet" href="/assets/katex/katex.min.css"><link rel="stylesheet" href="/assets/pseudocode/pseudocode.css" type="text/css"><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Anubhav Sachan" href="/atom.xml"><link rel="alternate" title="Anubhav Sachan" type="application/json" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /><style type="text/css"> @font-face{font-family:'Inter';font-style:normal;font-weight:100;font-display:swap;src:url("/assets/fonts/Inter-Thin.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Thin.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:100;font-display:swap;src:url("/assets/fonts/Inter-ThinItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ThinItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url("/assets/fonts/Inter-ExtraLight.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraLight.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:200;font-display:swap;src:url("/assets/fonts/Inter-ExtraLightItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraLightItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url("/assets/fonts/Inter-Light.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Light.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:300;font-display:swap;src:url("/assets/fonts/Inter-LightItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-LightItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url("/assets/fonts/Inter-Regular.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Regular.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:400;font-display:swap;src:url("/assets/fonts/Inter-Italic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Italic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url("/assets/fonts/Inter-Medium.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Medium.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:500;font-display:swap;src:url("/assets/fonts/Inter-MediumItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-MediumItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url("/assets/fonts/Inter-SemiBold.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-SemiBold.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:600;font-display:swap;src:url("/assets/fonts/Inter-SemiBoldItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-SemiBoldItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url("/assets/fonts/Inter-Bold.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Bold.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:700;font-display:swap;src:url("/assets/fonts/Inter-BoldItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-BoldItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:swap;src:url("/assets/fonts/Inter-ExtraBold.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraBold.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:800;font-display:swap;src:url("/assets/fonts/Inter-ExtraBoldItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraBoldItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:swap;src:url("/assets/fonts/Inter-Black.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Black.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:900;font-display:swap;src:url("/assets/fonts/Inter-BlackItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-BlackItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter var';font-weight:100 900;font-style:normal;font-named-instance:'Regular';font-display:swap;src:url("/assets/fonts/Inter-roman.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var';font-weight:100 900;font-style:italic;font-named-instance:'Italic';font-display:swap;src:url("/assets/fonts/Inter-italic.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var alt';font-weight:100 900;font-style:normal;font-named-instance:'Regular';font-display:swap;src:url("/assets/fonts/Inter-roman.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var alt';font-weight:100 900;font-style:italic;font-named-instance:'Italic';font-display:swap;src:url("/assets/fonts/Inter-italic.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var experimental';font-weight:100 900;font-style:oblique 0deg 10deg;font-display:swap;src:url("/assets/fonts/Inter.var.woff2?3.13") format("woff2")}html{font-family:'Inter', 'Helvetica', sans-serif;font-display:swap}@supports (font-variation-settings: normal){html{font-family:'Inter var', 'Helvetica', sans-serif}}body{font-size:1rem;line-height:1.5;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility}a,a:visited{color:inherit}a:hover,a:visited:hover{color:inherit}h1,h2,h3,h4,h5,nav a{font-weight:500}blockquote{background:#f9f9f9;border-left:5px solid black;font-size:120%;margin:2rem 0;padding:1rem}blockquote p{margin:0}blockquote footer{font-size:80%;margin:1rem 0 0 0}dl dt{margin-bottom:0.5rem}dl dd{font-style:italic;margin-bottom:2rem}code,.highlight{background:#edf2f7;overflow:auto}code{word-break:break-all}code{padding:0.1rem 0.3rem}pre{padding:1em}.date{opacity:0.6}html{box-sizing:border-box;margin:0;padding:0}*,*:before,*:after{box-sizing:inherit}body{background-color:#edf2f7;color:#495057}header,main{margin:0 auto;max-width:50rem}.grid{display:flex;flex-wrap:wrap;display:grid;grid-template-columns:1fr;grid-auto-rows:minmax(10rem, auto);grid-gap:1rem}.card{display:flex;align-items:left;justify-content:left;min-height:10rem;position:relative;margin-left:1rem;margin-right:1rem;flex:1 1 10rem}@supports (display: grid){.card{margin:0}}.card:nth-child(1n){background:#96d0c3}.card:nth-child(2n){background:#ef7d4d}.card:nth-child(3n){background:#fbcfb9}.card:nth-child(4n){background:#fac14e}.card:nth-child(5n+1){background:#ef7d4d}.card:nth-child(6n+1){background:#ef7d4d}.card:nth-child(7n+1){background:#fac14e}.card:nth-child(4n+1){background:#fac14e}.card:nth-child(4n+2){background:#fbcfb9}.card:nth-child(4n+3){background:#ef7d4d}.card:nth-child(4n+4){background:#96d0c3}.card{background:white;-webkit-box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);text-decoration:none;color:inherit}.card .thumb{padding-bottom:60%;background-size:cover;background-position:center center}.card a{text-decoration:none}.card p{font-size:0.9rem}.card h2{font-size:1.3rem;margin:0}.card time{font-size:0.8rem;font-weight:500;text-transform:uppercase;letter-spacing:.05em}.card .meta{padding:2rem;display:flex;flex:1;justify-content:space-between;flex-direction:column}.post{background:#fcfff9;padding:2rem 3rem;-webkit-box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03)}.card{transition:all 0.3s cubic-bezier(0.25, 0.45, 0.45, 0.95)}.card:hover{transform:scale(1.02)}ul,ol{padding:0;list-style-position:inside}table{border-collapse:collapse;text-align:left;width:100%}table tr{border-bottom:1px solid black}table td{padding:0.5rem}img{width:100%;margin:0.5rem 0}nav ul{list-style:none !important;padding:0;text-align:center}nav ul li{display:inline-block}nav a,nav a:visited{margin:0.5rem;text-decoration:none;text-transform:uppercase;color:inherit;font-size:0.8rem;font-weight:500;letter-spacing:.05em}footer{margin:1rem 0;text-align:center}.row{display:flex}.column{flex:33%}</style></head><body><header role="banner"><nav role="navigation"><ul><li><a href="/" ><b>Anubhav Sachan</b> &nbsp; | </a></li><li><a href="/about" >Timeline</a></li><li><a href="/experience" >Experience</a></li><li><a href="/blog" >Blog</a></li><li><a href="/projects" >Projects</a></li><li><a href="/assets/Anubhav_Sachan_CV.pdf">Resume</a></li><li><a href="/tags" >Tags</a></li></ul></nav></header><main id="main" role="main"> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><article class="post" role="article"><div class="meta"><h1>Model-based Offline Multi-Agent Dialogue Policy Learning</h1><time class="date" datetime="2020-06-15T00:00:00+05:30">June, 2020 &middot; <a href="https://github.com/anubhav4sachan/madpl" target="_blank">Github &#x2197;</a><br>Speech and Language Technology Lab, Saarthi.ai </time><div class="meta" style="text-align: justify;"><p>The conventional problem of selecting an action when provided with a state, environment and history (context) falls in the domain of self-play reinforcement learning. Numerous such algorithms learn a dialog policy with the reward function requiring an elaborate design of a comprehensive user simulator and pre-specified user goals. Here, in the implemented method, it is regarded that the user agent can learn with the system agent in a joint/shared fashion. The method involves the concept of role-aware reward decomposition using Hybrid Value Network with the integration of actor-critic framework to maximize the global reward for the policy learner.</p><h2 id="introduction">Introduction</h2><p>Dialog Policy plays a vital role in deciding the subsequent action for a conversation and hence, responsible for steering the dialog to the desired goal. The task oriented dialog systems, usually, aim at <em>concise</em> conversations.</p><p>The dialogue managers are frequently modeled as a sequential decision-making problem, where reinforcement learning (RL) is used for learning an optimal dialogue policy from user experiences. While a variety of RL methods have been developed for learning dialogue policies, the methods typically require a large amount of dialogue corpus until one can learn a good-quality dialogue policy. This method is quite infeasible, in the real world and shall be considered as a liability for a venture.</p><p>Offline learning focuses on learning the policies entirely from a batch of previously collected data. This problem setting is compelling, because it offers the promise of utilizing large, diverse, previously collected datasets to acquire policies without any costly or dangerous active exploration, but it is also exceptionally difficult, due to the distributional shift between the offline training data and the learned policy.</p><h2 id="multi-agent-dialogue-policy-learning">Multi-Agent Dialogue Policy Learning</h2><h3 id="the-architecture">The Architecture</h3><p>The learning method requires an annotated dialogue corpus <script type="math/tex">\mathcal{D}</script>. Specifically, the user goal must be defined as <script type="math/tex">\texttt{G = (C, R)}</script> where <script type="math/tex">\texttt{C}</script> represents the constraints and <script type="math/tex">\texttt{R}</script> corresponds to the requests. There also exists an external database <script type="math/tex">\texttt{DB}</script>, which can accessed to provide the required information and fulfil the requests. <script type="math/tex">\texttt{G}</script> can have multiple domains, and both the agents accomplish the sub-tasks in each of the domain. The agents are in a partially observable environment, hence, corresponds to Partially Observable Markov Decision Process. Precisely, the user agent has access to only <script type="math/tex">\texttt{G}</script> and the system agent has access to the <script type="math/tex">\texttt{DB}</script>. The only way to gain information from each other is to engage in an interaction through dialogues.</p><center><figure> <img src="/assets/images/architecture.png" alt="architecture" style="width: 80%;" /><figcaption style="font-size: 10pt;">The basic architecture for Dialogue Policy Learning.</figcaption></figure></center><p>The designed architecture, for task-oriented dialogue systems, has reward decomposition for the user as well as the system agent during the conversation. The user and the system interact with each other as dialogue agents to jointly learn the policy. They can interact with each other, only and only through an abstract representation of an intention called a <em>dialogue act</em>. It is, in our case, represented in a quadruple of form <em>[domain, intent, slot-type, slot-value]</em>.</p><p>The algorithm is largely based on the Actor Critic Framework following the paradigm of centralized training and decentralized execution in multi agent reinforcement learning. It basically asserts that the actor selects its action conditioned on local states, actions for a particular agent whereas the critic is based on all the agents. The Hybrid Value Network consists of three critics in which each critic estimates its return based on role-aware reward decomposition, and each actor uses the estimated value to optimize itself.</p><h3 id="dialogue-policies-and-reward-distribution">Dialogue Policies and Reward Distribution</h3><h4 id="dialogue-policies">Dialogue Policies:</h4><p><strong><em>System Policy:</em></strong> The system policy <script type="math/tex">\pi</script> decides the system action <script type="math/tex">a^S</script> according to the system dialog state <script type="math/tex">s^S</script> to give an appropriate response the user agent. Each system action is a subset of dialogue act set <script type="math/tex">\mathcal{A}</script> as there may be multiple intents a dialog turn. In practice, dialog acts are de-lexicalized in the dialog policy. The slot values are replaced with a count placeholder and is refilled with the true value according to the entity selected from the external database <script type="math/tex">\texttt{DB}</script>. This faciliates the system to operate on unseen values.</p><p>The system dialog state <script type="math/tex">s^S</script> at dialog turn <em>t</em> is the concatenation of:</p><ul><li>User action at current turn <script type="math/tex">a^U_t</script>;</li><li>System action <script type="math/tex">a^S_{t-1}</script> at the last turn;</li><li>The belief state <script type="math/tex">b_t</script> that keeps track of constraint &amp; request slots given by user agent, and</li><li>Embedding vectors of the number of query results <script type="math/tex">q_t</script> from <script type="math/tex">\texttt{DB}</script>.</li></ul><p><strong><em>User Policy:</em></strong> The user policy <script type="math/tex">\mu</script> decides the user action <script type="math/tex">a^U</script> according to the user dialog state <script type="math/tex">s^U</script> to express the constraints (<script type="math/tex">\texttt{C}</script>) and requests (<script type="math/tex">\texttt{R}</script>) to the system agent.</p><p>The user dialog state <script type="math/tex">s^U</script> at dialog turn <em>t</em> is the concatenation of:</p><ul><li>Last system action <script type="math/tex">a^S_{t-1}</script>;</li><li>Last user action <script type="math/tex">a^U_{t-1}</script>;</li><li>The goal state <script type="math/tex">g_t</script>, and;</li><li>An inconsistency vector <script type="math/tex">c_t</script> to indicate the inconsistency between system response and user constraint.</li></ul><p>In addition to the prediction of dialogue acts, user policy also predicts the terminal signal <em>T</em> at the same time, such that <script type="math/tex">\mu = \mu (a^U, T \mid s^U)</script>.</p><h4 id="rewards-distribution">Rewards Distribution:</h4><p>The roles of both the agents, user as well as the system are quite different. The user agent actively initiates a task and may change it during conversation, but the system agent passively responds to the user agent and returns the proper information, so the reward should be considered separately for each agent. On the other hand, two agents communicate and collaborate to accomplish the same task cooperatively, so the reward also involves a global target for both agents. Hence, three fold reward distribution is considered as described.</p><ul><li><strong><em>System Reward</em></strong> (<script type="math/tex">r^S_t</script>) consists of:<ul><li>Empty dialog act penalty <script type="math/tex">a^S_t = \varnothing</script>;</li><li>Late answer penalty, if there is a request slot triggered but the system agent does not reply the information immediately, and;</li><li>Task success reward based on the user agent’s description.</li></ul></li><li><strong><em>User Reward</em></strong> (<script type="math/tex">r^U_t</script>) consists of:<ul><li>Empty dialog act penalty <script type="math/tex">a^U_t = \varnothing</script>;</li><li>Early inform penalty, and;</li><li>User goal reward, whether the user agents have expressed all the constraints <script type="math/tex">\texttt{C}</script> and requests <script type="math/tex">\texttt{R}</script>.</li></ul></li><li><strong><em>Global Reward</em></strong> (<script type="math/tex">r^G_t</script>) consists of:<ul><li>Efficiency penalty, that a small negative value will be given at each dialog turn;</li><li>Sub-goal completion reward once the sub-task of <script type="math/tex">\texttt{G}</script> in a particular domain is accomplished, and;</li><li>Task success reward based on user goal <script type="math/tex">\texttt{G}</script>.</li></ul></li></ul><p>Each agent obtains its <em>local reward</em> during each turn, and the <em>global reward</em> is awarded during the training of the model architecture.</p><h3 id="hybrid-value-network">Hybrid Value Network</h3><p>The aim of the value network is to estimate the expected return given a current state so that the policy can directly use the estimated cumulative reward for optimization, without sampling the trajectories to obtain rewards which may cause high variance. The actor critic approach in a multi agent reinforcement learning can easily be integrated with centralized training and decentralized execution framework i.e. actor of each agent benefits from a critic and the critic is augmented with the information from all the agents during the training phase.</p><p>The Hybrid Value Network (HVN) is inspired from Hybrid Reward Architecture for Reinforcement Learning. HVN improves the estimate of optimum role aware value function.</p><p>It first encodes the dialog state of each agent to learn a state representations:</p><center> $$\begin{equation} h^S_s = tanh(f^S_s (s^S)), \label{eq:hybrid_sys} \end{equation}$$ $$\begin{equation} h^U_s = tanh(f^U_s (s^U)) \label{eq:hybrid_usr} \end{equation}$$ </center><p>where <script type="math/tex">f (.)</script> can be any neural network unit. The value network is separated into branches each for the value of user reward (<script type="math/tex">V^U(s^U)</script>), system reward (<script type="math/tex">V^S(s^S)</script>) and global reward (<script type="math/tex">V^G(s)</script>).</p><h2 id="algorithm">Algorithm</h2><pre id="read-2" style="display:none;">
    
\begin{algorithm}
\caption{MADPL}
\begin{algorithmic}
\PROCEDURE{Pre-Training}{}
\STATE $\textbf{require:}$ Human to System Conversation Dialog Corpus $\mathcal{D}$ with the annotations of dialog acts $a$
    \STATE initialize: weights $\phi$, $\omega$ for system policy $\pi$ \&amp; user policy $\mu$ respectively
    \STATE pre-train: policies $\pi$, $\mu$ on $\mathcal{D}$ using equation 1.
\ENDPROCEDURE

\PROCEDURE{Training}{}
    \STATE initialize: the weights $\theta$ for the Hybrid Value Network ($V = V^S, V^U, V^G$) and target network $\theta^- \leftarrow \theta$
    \STATE $n = $ training iterations
    \FOR{$j = 0$ \TO $n$}
        \STATE initialize: user goal and dialog state $s^U$, $s^S$
        \STATE $\textbf{repeat:}$
        \STATE Sample actions $a^U$, $a^S$ and terminal signal $\mathcal{T}$ using current policy $\pi$, $\mu$
        \STATE Execute actions and observe reward $r^U$, $r^S$, $r^G$ and new states $s'^U$, $s'^S$
        \STATE Update hybrid value network (critic) using Eq. vhnet
        \STATE Compute the advantage $A^U$, $A^S$, $A^G$ using current value network
        \STATE Update two dialog policies (actor)
        \STATE Assign target network parameters $\theta^- \leftarrow \theta$ every C steps
        \STATE $\textbf{end repeat}$ at $\mathcal{T}$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}

</pre><div id="goal-2"></div><script type="text/javascript"> var code = document.getElementById("read-2").textContent; var parentEl = document.getElementById("goal-2"); var options = { lineNumber: true }; pseudocode.render(code, parentEl, options); </script><h3 id="pre-training-session">Pre-Training Session</h3><p>The initial two steps of the algorithm define the pre-training session. This involves the offline model-based training of the system policy ($\pi$) and the user policy ($\mu$) on human dialogue corpus $\mathcal{D}$ annotated with dialogue acts $\mathcal{a}$ according to the loss function as expressed in the equation 1.</p><center><figure> $$\begin{equation} L\hspace{2pt} (X, Y; \beta) = - [\beta \cdot Y^T \cdot log \hspace{2pt} \sigma (X) + (I - Y)^T \cdot log \hspace{2pt} (I - \sigma (X))] \label{eq:pretrain_loss} \end{equation}$$<figcaption style="font-size: 10pt;">Equation 1</figcaption></figure></center><p>Here, X is the state and Y is the action from the corpus of the given task. The <script type="math/tex">\beta</script>-weighted logistic regression for policy pre-training here to alleviate data bias because each agent only generates several dialog acts in one dialog turn.</p><h3 id="training-session">Training Session</h3><p>The training of the algorithm is split into two stages, first, the pre-training stage and the second stage is the optimization process of both user as well as system policy.</p><p>For the critic optimization, it aims to minimize the squared error between the temporal difference target (<script type="math/tex">r_t + \gamma V(s_{t+1})</script>) and the estimated value (<script type="math/tex">V(s_t) = \mathbb{E} \hspace{2pt}[r_t + \gamma V(s_{t+1})]</script>). Due to the high variance in framework, the target network is introduced to bring stability in the training process. The actor-critic framework has high variance, since the critic part is updated too frequently and hence the changes done in estimated value bring instability in the training process. This is quite prominent in Multi Agent Reinforcement Learning.</p><p>In context of hybrid value network the session focuses on the minimization of loss functions described in equation 2.</p><center><figure> $$ \begin{equation} L^S_V (\theta) = (r^S + \gamma V^S_{\theta^-} (s'^S) - V^S_\theta (s^S))^2, \end{equation}$$ $$ \begin{equation} L^U_V (\theta) = (r^U + \gamma V^U_{\theta^-} (s'^U) - V^U_\theta (s^U))^2, \end{equation}$$ $$ \begin{equation} L^G_V (\theta) = (r^G + \gamma V^G_{\theta^-} (s'^G) - V^G_\theta (s^G))^2, \end{equation}$$ $$ \begin{equation} L_V = L^S_V + L^U_V + L^G_V \label{hvnet} \end{equation}$$<figcaption style="font-size: 10pt;">Equation 2</figcaption></figure></center><p><script type="math/tex">V_\theta</script> is parameterized by <script type="math/tex">\theta</script> and <script type="math/tex">\theta^-</script> is the weight of the target network. The overall loss is the sum of value estimation loss on each component reward. Each dialogue policy aims to maximize for the related returns i.e. system policy aims to maximize the cumulative system awards and global rewards, and similarly, the user policy.</p><p>The advantage (<script type="math/tex">A(s) = r + \gamma V(s') - V(s)</script>) estimated by the critic can evaluate the new state and the current state to determine the quality of the dialogue (better or worse). With the aid of HVN, the sum of the related component advantages can be used to update different agents.</p><h2 id="conclusion">Conclusion</h2><p>The new approach introduced in the paper Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition is quite insightful, and can be used for highly improved dialogue policy. The approach can be applied to leading datasets like DSTC8 - Schema Guided Dataset, to achieve state of the art results. There lies a great opportunity in improving the metrics viz. Inform F1 and the Match Rate, with the modification of the basic neural networks, to more complex networks, and using a better optimizer like AdamW. This project has been implemented using PyTorch Framework.</p><time class="date"> Reinforcement Learning, Dialogue Policy Learning, Natural Language Processing</time></article></main><footer class="footer" role="contentinfo"> <small> &copy; 2020, <a href="/">Anubhav Sachan</a> <br>Social: <a href = "https://www.github.com/anubhav4sachan/" target="_blank">Github</a>, <a href="https://www.twitter.com/anubhav4sachan/" target="_blank">Twitter</a>, <a href = "https://www.linkedin.com/in/anubhav4sachan/" target="_blank">LinkedIn</a> &amp; <a href="https://www.instagram.com/anubhavenue/" target="_blank">Instagram</a>.<br>Feel free to reach via <a href = "mailto:hi@anubhavsachan.com">mail</a>. </small></footer></body></html>
