<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Intuitive Neural Networks | Anubhav Sachan</title><meta name="generator" content="Jekyll v3.8.7" /><meta property="og:title" content="Intuitive Neural Networks" /><meta name="author" content="Anubhav Sachan" /><meta property="og:locale" content="en_US" /><meta name="description" content="With pretty much everything going on with neural networks, it is high time to understand the logical intuitiveness with minimal math behind them." /><meta property="og:description" content="With pretty much everything going on with neural networks, it is high time to understand the logical intuitiveness with minimal math behind them." /><link rel="canonical" href="http://localhost:4000/2020/08/29/intuitive-nn" /><meta property="og:url" content="http://localhost:4000/2020/08/29/intuitive-nn" /><meta property="og:site_name" content="Anubhav Sachan" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-08-29T00:00:00+05:30" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Intuitive Neural Networks" /><meta name="twitter:site" content="@anubhav4sachan" /><meta name="twitter:creator" content="@Anubhav Sachan" /> <script type="application/ld+json"> {"@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/08/29/intuitive-nn"},"url":"http://localhost:4000/2020/08/29/intuitive-nn","author":{"@type":"Person","name":"Anubhav Sachan"},"headline":"Intuitive Neural Networks","dateModified":"2020-08-29T00:00:00+05:30","description":"With pretty much everything going on with neural networks, it is high time to understand the logical intuitiveness with minimal math behind them.","datePublished":"2020-08-29T00:00:00+05:30","@context":"https://schema.org"}</script> <script src="/assets/katex/katex.min.js"></script> <script src="/assets/pseudocode/pseudocode.js" type="text/javascript"></script><link rel="stylesheet" href="/assets/katex/katex.min.css"><link rel="stylesheet" href="/assets/pseudocode/pseudocode.css" type="text/css"><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Anubhav Sachan" href="/atom.xml"><link rel="alternate" title="Anubhav Sachan" type="application/json" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /><style type="text/css"> @font-face{font-family:'Inter';font-style:normal;font-weight:100;font-display:swap;src:url("/assets/fonts/Inter-Thin.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Thin.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:100;font-display:swap;src:url("/assets/fonts/Inter-ThinItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ThinItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url("/assets/fonts/Inter-ExtraLight.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraLight.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:200;font-display:swap;src:url("/assets/fonts/Inter-ExtraLightItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraLightItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url("/assets/fonts/Inter-Light.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Light.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:300;font-display:swap;src:url("/assets/fonts/Inter-LightItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-LightItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url("/assets/fonts/Inter-Regular.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Regular.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:400;font-display:swap;src:url("/assets/fonts/Inter-Italic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Italic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url("/assets/fonts/Inter-Medium.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Medium.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:500;font-display:swap;src:url("/assets/fonts/Inter-MediumItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-MediumItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url("/assets/fonts/Inter-SemiBold.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-SemiBold.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:600;font-display:swap;src:url("/assets/fonts/Inter-SemiBoldItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-SemiBoldItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url("/assets/fonts/Inter-Bold.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Bold.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:700;font-display:swap;src:url("/assets/fonts/Inter-BoldItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-BoldItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:swap;src:url("/assets/fonts/Inter-ExtraBold.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraBold.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:800;font-display:swap;src:url("/assets/fonts/Inter-ExtraBoldItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraBoldItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:swap;src:url("/assets/fonts/Inter-Black.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Black.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:900;font-display:swap;src:url("/assets/fonts/Inter-BlackItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-BlackItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter var';font-weight:100 900;font-style:normal;font-named-instance:'Regular';font-display:swap;src:url("/assets/fonts/Inter-roman.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var';font-weight:100 900;font-style:italic;font-named-instance:'Italic';font-display:swap;src:url("/assets/fonts/Inter-italic.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var alt';font-weight:100 900;font-style:normal;font-named-instance:'Regular';font-display:swap;src:url("/assets/fonts/Inter-roman.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var alt';font-weight:100 900;font-style:italic;font-named-instance:'Italic';font-display:swap;src:url("/assets/fonts/Inter-italic.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var experimental';font-weight:100 900;font-style:oblique 0deg 10deg;font-display:swap;src:url("/assets/fonts/Inter.var.woff2?3.13") format("woff2")}html{font-family:'Inter', 'Helvetica', sans-serif;font-display:swap}@supports (font-variation-settings: normal){html{font-family:'Inter var', 'Helvetica', sans-serif}}body{font-size:1rem;line-height:1.5;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility}a,a:visited{color:inherit}a:hover,a:visited:hover{color:inherit}h1,h2,h3,h4,h5,nav a{font-weight:500}blockquote{background:#f9f9f9;border-left:5px solid black;font-size:120%;margin:2rem 0;padding:1rem}blockquote p{margin:0}blockquote footer{font-size:80%;margin:1rem 0 0 0}dl dt{margin-bottom:0.5rem}dl dd{font-style:italic;margin-bottom:2rem}code,.highlight{background:#edf2f7;overflow:auto}code{word-break:break-all}code{padding:0.1rem 0.3rem}pre{padding:1em}.date{opacity:0.6}html{box-sizing:border-box;margin:0;padding:0}*,*:before,*:after{box-sizing:inherit}body{background-color:#edf2f7;color:#495057}header,main{margin:0 auto;max-width:50rem}.grid{display:flex;flex-wrap:wrap;display:grid;grid-template-columns:1fr;grid-auto-rows:minmax(10rem, auto);grid-gap:1rem}.card{display:flex;align-items:left;justify-content:left;min-height:10rem;position:relative;margin-left:1rem;margin-right:1rem;flex:1 1 10rem}@supports (display: grid){.card{margin:0}}.card:nth-child(1n){background:#96d0c3}.card:nth-child(2n){background:#ef7d4d}.card:nth-child(3n){background:#fbcfb9}.card:nth-child(4n){background:#fac14e}.card:nth-child(5n+1){background:#ef7d4d}.card:nth-child(6n+1){background:#ef7d4d}.card:nth-child(7n+1){background:#fac14e}.card:nth-child(4n+1){background:#fac14e}.card:nth-child(4n+2){background:#fbcfb9}.card:nth-child(4n+3){background:#ef7d4d}.card:nth-child(4n+4){background:#96d0c3}.card{background:white;-webkit-box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);text-decoration:none;color:inherit}.card .thumb{padding-bottom:60%;background-size:cover;background-position:center center}.card a{text-decoration:none}.card p{font-size:0.9rem}.card h2{font-size:1.3rem;margin:0}.card time{font-size:0.8rem;font-weight:500;text-transform:uppercase;letter-spacing:.05em}.card .meta{padding:2rem;display:flex;flex:1;justify-content:space-between;flex-direction:column}.post{background:#fcfff9;padding:2rem 3rem;-webkit-box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03)}.card{transition:all 0.3s cubic-bezier(0.25, 0.45, 0.45, 0.95)}.card:hover{transform:scale(1.02)}ul,ol{padding:0;list-style-position:inside}table{border-collapse:collapse;text-align:left;width:100%}table tr{border-bottom:1px solid black}table td{padding:0.5rem}img{width:100%;margin:0.5rem 0}nav ul{list-style:none !important;padding:0;text-align:center}nav ul li{display:inline-block}nav a,nav a:visited{margin:0.5rem;text-decoration:none;text-transform:uppercase;color:inherit;font-size:0.8rem;font-weight:500;letter-spacing:.05em}footer{margin:1rem 0;text-align:center}.row{display:flex}.column{flex:33%}</style></head><body><header role="banner"><nav role="navigation"><ul><li><a href="/" ><b>Anubhav Sachan</b> &nbsp; | </a></li><li><a href="/about" >Timeline</a></li><li><a href="/experience" >Experience</a></li><li><a href="/blog" >Blog</a></li><li><a href="/projects" >Projects</a></li><li><a href="/assets/Anubhav_Sachan_CV.pdf">Resume</a></li><li><a href="/tags" >Tags</a></li></ul></nav></header><main id="main" role="main"> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><article role="article" class="post"><div class="post-meta"><h1>Intuitive Neural Networks</h1><time class="date" datetime="2020-08-29T00:00:00+05:30">August 29, 2020 &middot; <a href="https://medium.com/datadriveninvestor/intuitive-neural-networks-cb4f6e1c9aa0" target="_blank">Medium &#x2197;</a></time></div><p>Machine learning is a field of study which focuses on the improvement of the performance measure through experiences for some specific tasks. With the introduction of LeNet by Prof. LeCun on the classification of handwritten digits (MNIST) dataset in late 90s, people in research community came to an understanding that neural networks with more than one hidden layers can achieve state of the art (SoTA) results (which was a thought not much appreciated by researchers back then). The modern advances in deep learning methods are facilitated due to the presence of large amount of data and computational power provided through the GPUs and hence giving rise to huge networks like GPTs (GPT-3, being most recent), ResNet, VGG etc. In this article, we will be focusing on the foundational idea of Neural Networks on which these SoTA architectures are built upon.</p><h1 id="neurons">Neurons</h1><p>A brain, in general, harnesses the network of neurons to perform dozens of complex tasks efficiently. Every neuron processes signals entering from a dendrite and gives an output which is sent to the other neuron(s) and processed upon further. Hence, we can evidently state that removal of the network or even the reduction in the complexity of networks can harm its decision making process.</p><center><figure> <img src="/assets/images/mnist_snap.png" alt="mnist_snap" style="width: 30%;" /><figcaption style="font-size: 10pt;">A snapshot from MNIST Dataset.</figcaption></figure></center><p>The handwritten digits, for instance, are easily interpreted by the brain to differentiate between the digits, but if we have to write a program to classify them, it would be quite an arduous task. To solve this task, we take some inspiration from these dynamically activating neurons, and formulate a way to mimic it. The whole idea of Machine Learning and Artificial Intelligence is to develop a computer program comprising of artificial neurons (because, neurons are the most powerful entities to perform complex tasks) which shall eventually outperform the capabilities of a human brain.</p><p>The impulses from the sensory parts of the body reach the dendrite and if they are strong enough to create a stimulation, the axon outputs a spike. Since, biological neurons are living cells, they can modify themselves to define a stimulation threshold and hence are dynamic in nature.</p><center><figure> <img src="/assets/images/bio_neuron.png" alt="bio_neuron" style="width: 60%;" /><figcaption style="font-size: 10pt;">A biological neuron.</figcaption></figure><figure> <img src="/assets/images/artificial_neuron.png" alt="artificial_neuron" style="width: 70%;" /><figcaption style="font-size: 10pt;">An artificial neuron.</figcaption></figure></center><p>Similarly, the artificial neuron has some inputs and each of the input (<script type="math/tex">x_i</script>) is attached to a weight (<script type="math/tex">w_i</script>) and bias (<script type="math/tex">b</script>) (which are learned during their training period). The weight influences the importance of particular input. They perform a computation and produce a signal, which is forwarded through an activation function (<script type="math/tex">f</script>), to produce an output spike (<script type="math/tex">y</script>), given that such signal from <script type="math/tex">f</script> is above the threshold value.</p><p>To illustrate, let’s consider the MNIST handwritten digits. Each digit is <script type="math/tex">28\times28</script> px in size and each pixel has a grayscale value lying in the range of [0, 1] where 0 &amp; 1 represent black and white respectively. The 2-D array is reshaped to a single dimensional array <script type="math/tex">x</script> of length <em>784</em>, and each index corresponds to an input pixel (<script type="math/tex">x_i</script>). We know that the black/dark pixels do not contribute to the curves/textures, hence they have less importance and light/white pixels exhibit quite significance in the determination of the digit (label). The relevance of a specific pixel (termed as a feature) is determined by the weights attached.</p><h1 id="single-layer-network">Single Layer Network</h1><center><figure> $$\begin{equation} y = f(\sum x_i\cdot w_i + b) \label{Eq:linearregression} \end{equation}$$<figcaption style="font-size: 10pt;">Eq. 1</figcaption></figure></center><p>Equation 1, mathematically, describes the basic functioning of a single layer network. It also establishes its relationship with a biological neuron. Precisely, the output $y$ is a function of an affine transformation of input features, characterized by a linear transformation of features via weighted sum, combined with a translation via the added bias.</p><p>The major goal of the single layer model lies in the identification of a set of weights (<script type="math/tex">w_i</script>) for corresponding input (<script type="math/tex">x_i</script>) so as to fit the data efficiently for our predicted output (<script type="math/tex">\hat{y}</script>). This will ensure a generalized behaviour over the dataset.</p><p>Ignoring the function <script type="math/tex">f</script> for a while, we understand that</p><center><figure> $$\begin{equation} \hat{y} = x_1 \cdot w_1 + x_2 \cdot w_2 + ... + x_d \cdot w_d + b \label{eq:expandedlr} \end{equation}$$<figcaption style="font-size: 10pt;">Eq. 2</figcaption></figure></center><p>where <script type="math/tex">d</script> is total number of features from a input<script type="math/tex">{^1}</script>, <script type="math/tex">w_i</script> are the required weights. If we collect all the features into a vector <script type="math/tex">\textbf{x} \in \mathbb{R}^d</script> and all our weights into another vector <script type="math/tex">\textbf{w} \in \mathbb{R}^d</script>, the Equation 2 can be simplified using a dot product:</p><center><figure> $$\begin{equation} \hat{y} = \textbf{w}^T \textbf{x} + b \label{eq:compactoned} \end{equation}$$<figcaption style="font-size: 10pt;">Eq. 3</figcaption></figure></center><p>The <script type="math/tex">\textbf{x}</script> in Equation 3 corresponds to features of a single data point. To express <script type="math/tex">\textbf{x}</script> for all the inputs (in the dataset) in a <script type="math/tex">\mathbb{R}^{n \times d}</script> dimensional space, we introduce a design matrix <script type="math/tex">\textbf{X} \in \mathbb{R}^{n \times d}</script> where each row corresponds to an example and every column for a particular feature.</p><p>Hence, equation 3 can be rewritten for <script type="math/tex">n \times d</script> dimensional space as:</p><center><figure> $$\begin{equation} \hat{y} = \textbf{X}\cdot \textbf{w} + b \label{eq:compactnd} \end{equation}$$<figcaption style="font-size: 10pt;">Eq. 4</figcaption></figure></center><p>and the search for the best parameters weights vector <script type="math/tex">\textbf{w}</script> and bias <script type="math/tex">b</script> lies in the objective function (quality measure) and the procedure to update the parameters for the improvement in quality.</p><p><script type="math/tex">\circ</script> Fun Fact : The vectorized equations (such as in Eq. \ref{eq:compactnd}) simplify the math and make sure the code runs faster<script type="math/tex">^2</script>. In fact, a GPU has a lot more cores than a standard CPU (around 4000, in comparison to CPU’s 4 cores) and this allows the multi-threading processes to work efficiently, since computation in each cell of the matrix is independent of other cells.</p><h1 id="multi-layer-perceptron">Multi-layer Perceptron</h1><p>As the name suggests, we, now, have more than one layer for our deep learning neural architecture. We described about the affine transformation (linear transformation with translation) in the previous section and described how an output from a single layered network is produced. In this section, we will dive deep into multi layer perceptron.</p><h2 id="hidden-layers">Hidden Layers</h2><p>The linear models are based on a strong assumption that the a single affine transformation can map our input data to the outputs which is quite unrealistic. In addition, linearity implies monotonicity i.e. increase in the inputs eventually will either cause increase or decrease in the outputs. Let’s think about the classification of digits, the increase in intensity of a pixel doesn’t imply the increase in probability of getting a digit of higher magnitude. Hence, this assumption will surely fail in the case of image data (and of course, various other cases).</p><p>We have understood that the relevance of each input feature (pixel) is more complex<script type="math/tex">^3</script> than expected. So, we introduce a few more fully connected (dense) layers between the inputs and output(s) which are termed as hidden layers. This architecture is referred to as Multi-layer Perceptron (MLP).</p><center><figure> <img src="/assets/images/mlp.jpeg" alt="mlp" style="width: 72%;" /><figcaption style="font-size: 10pt;">Illustration of Multi-layer Perceptron with a hidden layer of 16 neurons.</figcaption></figure></center><p>Taking up our classic example of MNIST Dataset, we will define a single hidden layer with 16 neurons as shown in Figure of MLP above. The figure lucidly explains the 784 input features, a hidden layer (more such fully-connected (dense) layers can be stacked, with any number of neurons), and output layer. Take a note that, neither input nor output layer is considered to be hidden.</p><p>Previously, we defined the input matrix as <script type="math/tex">\textbf{X} \in \mathbb{R}^{n \times d}</script> where <script type="math/tex">n</script> &amp; <script type="math/tex">d</script> are number of examples and features respectively. For a one-hidden-layer MLP, with <script type="math/tex">h</script> neurons (hidden units), we can define a hidden layer matrix <script type="math/tex">\textbf{H} \in \mathbb{R}^{n \times h}</script>. Since the hidden and output layers are both fully connected, we have hidden-layer weights and biases as <script type="math/tex">\textbf{W}^{(1)} \in \mathbb{R}^{d \times h}</script> and <script type="math/tex">\textbf{b}^{(1)} \in \mathbb{R}^{1 \times h}</script> and output layer weights and biases as <script type="math/tex">\textbf{W}^{(2)} \in \mathbb{R}^{h \times c}</script> and <script type="math/tex">\textbf{b}^{(2)} \in \mathbb{R}^{1 \times c}</script>. We have chosen simple (1) for first layer (the hidden layer) and (2) for second layer (the output layer) and <script type="math/tex">c</script> is the number of classes. For our MNIST example: <script type="math/tex">n = 1, d = 784, h = 16, c = 10</script>.</p><p>Mathematically:</p><figure> <center> $$\begin{equation} \textbf{H} = \textbf{X}\cdot \textbf{W}^{(1)} + \textbf{b}^{(1)}, \textbf{O} = \textbf{H}\cdot \textbf{W}^{(2)} + \textbf{b}^{(2)} \label{eq:hoeq} \end{equation}$$<figcaption style="font-size: 10pt;">Eq. 5</figcaption></center></figure><p>which can be rewritten as</p><figure> <center>$$\begin{equation} \textbf{O} = (\textbf{X}\cdot \textbf{W}^{(1)} + \textbf{b}^{(1)}) \cdot \textbf{W}^{(2)} + \textbf{b}^{(2)} = \textbf{X}\cdot \textbf{W}^{(1)}\textbf{W}^{(2)} + \textbf{b}^{(1)}\textbf{W}^{(2)} + \textbf{b}^{(2)} = \textbf{X}\cdot \textbf{W} + \textbf{b}. \label{eq:hoopened} \end{equation}$$<figcaption style="font-size: 10pt;">Eq. 6</figcaption></center></figure><p>where <script type="math/tex">\textbf{W} = \textbf{W}^{(1)}\textbf{W}^{(2)}</script> and <script type="math/tex">\textbf{b} = \textbf{b}^{(1)}\textbf{W}^{(2)} + \textbf{b}^{(2)}</script>.</p><p>The end result (<script type="math/tex">\textbf{O} = \textbf{X}\cdot \textbf{W} + \textbf{b}</script>) brings us back to a linear layer which practically equivalent to Equation 1. This means stacking linear layers over one another will again establish the linear behavior and will act as if only a single layer is present.</p><p>To inculcate a non-linear behaviour, each neuron in a hidden layer is subjected to an activation function <script type="math/tex">f</script>, and its outputs are referred to as activations. This activation function brings in a non-linearity and facilitates the MLP architecture to not fall back into a linear model. The equation 5 can be rewritten as: <script type="math/tex">\begin{equation} \textbf{H} = f(\textbf{X}\cdot \textbf{W}^{(1)} + \textbf{b}^{(1)}), \textbf{O} = \textbf{H}\cdot \textbf{W}^{(2)} + \textbf{b}^{(2)} \label{eq:hof} \end{equation}</script></p><p>The activation function (<script type="math/tex">f</script>) brings up the required non-linearity, and hence, causes the stack of linear layers to be non-linear. This can be brought from various activation functions as shown in the given figure.</p><center><figure> <img src="/assets/images/activationfxn.jpg" alt="activationfxn" style="width: 75%;" /><figcaption style="font-size: 10pt;">Commonly used non-linearities/activation functions ($$f$$).</figcaption></figure></center><p>This addition of non-linear layers increase the number of parameters in the neural network and hence, making it quite easy for the network to map any input with its output.</p><h1 id="conclusion">Conclusion</h1><p>With above illustration and simple mathematics, we understood how Single Layer and Multi Layer Perceptrons function. We also looked into how GPUs facilitate the neural networks and how addition of non-linearity boosts the neural network architecture.</p><h3 id="footnotes">Footnotes</h3><p><script type="math/tex">^1</script> <script type="math/tex">d</script> is chosen (instead of an obvious choice <script type="math/tex">n</script>) due to the fact that all the features can be visualized in a <script type="math/tex">d-</script>dimensional space. <script type="math/tex">n</script> is, however, used to denote the count of all the examples in a dataset.</p><p><script type="math/tex">^2</script> <a href="https://ai.stackexchange.com/questions/21938/how-do-gpus-faciliate-the-training-of-a-deep-learning-architecture">AI Stack Exchange Link: How do GPUs facilitate the training of a Deep Learning Architecture?</a></p><p><script type="math/tex">^3</script> For instance, it may depend on the surrounding pixels (referred to as \textit{context}), like in the construction of a straight line.</p><br><p> Tagged: <a href="/tag/neural-networks">#neural-networks</a>, <a href="/tag/deep-learning">#deep-learning</a>, <a href="/tag/intution">#intution</a>.</p><hr style="height:2px; border:none; margin: 2rem 0; background-color:#e7e9ee;"><div id="disqus_thread"></div><script> var disqus_name = "your disqus username"; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_name + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); </script> <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></article></main><footer class="footer" role="contentinfo"> <small> &copy; 2020, <a href="/">Anubhav Sachan</a> <br>Social: <a href = "https://www.github.com/anubhav4sachan/" target="_blank">Github</a>, <a href="https://www.twitter.com/anubhav4sachan/" target="_blank">Twitter</a>, <a href = "https://www.linkedin.com/in/anubhav4sachan/" target="_blank">LinkedIn</a> &amp; <a href="https://www.instagram.com/anubhavenue/" target="_blank">Instagram</a>.<br>Feel free to reach via <a href = "mailto:hi@anubhavsachan.com">mail</a>. </small></footer></body></html>
