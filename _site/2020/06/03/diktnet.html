<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <script src="//assets/katex/katex.min.js"></script>
  <script src="//assets/pseudocode/pseudocode.js" type="text/javascript"></script>
  <link rel="stylesheet" href="//assets/katex/katex.min.css">
  <link rel="stylesheet" href="//assets/pseudocode/pseudocode.css" type="text/css">

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Goal-Oriented Dialog Generation with Few-Shot Training & Knowledge Transfer &middot; Anubhav Sachan
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">

  <link rel="stylesheet" href="/public/css/font-awesome.min.css">
  <link rel="stylesheet" href="/public/css/font-awesome.css">

 <!--  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"> -->
 <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;700&display=swap" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0c">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <p style="font-size:46px; color: #fff;">
          Anubhav <b>Sachan</b>
      </p>
      <p style="font-size:20px;">
          Learning with<br>experience replay! <i class="fa fa-star"></i>
      </p>
      <p></p>
    </div>
    <hr width="90">
    <nav class="sidebar-nav" style="font-size: 90%">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/blog/">Blog</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/experience/">Experience</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/projects/">Projects</a>
          
        
      

<!--       <a class="sidebar-nav-item" href="https://github.com/anubhav4sachan//archive/v2.1.0.zip">Download</a>
      <a class="sidebar-nav-item" href="https://github.com/anubhav4sachan/">GitHub project</a>
      <span class="sidebar-nav-item">Currently v2.1.0</span> -->
      <a class="sidebar-nav-item" href="http://localhost:4000/assets/Anubhav_Sachan_CV.pdf">Resume</a>
    </nav>

<a href="https://www.github.com/anubhav4sachan/" target="_blank"><i class="fa fa-github"></i></a> &middot; <a href="https://www.linkedin.com/in/anubhav4sachan/" target="_blank"><i class="fa fa-linkedin-square"></i></a> &middot; <a href="https://www.twitter.com/anubhav4sachan/" target="_blank"><i class="fa fa-twitter"></i></a> &middot; <a href="https://www.medium.com/@anubhav4sachan/" target="_blank"><i class="fa fa-medium"></i></a> &middot; <a href="https://instagram.com/anubhavenue/" target="_blank"><i class="fa fa-instagram"></i></a> &middot; <a href="mailto:hi@anubhavsachan.com" target="_blank"><i class="fa fa-envelope-o"></i></a>
<br><br>
    <p style="font-size: 50%">&copy; 2020 | Built with <i class="fa fa-heart"></i> by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> &amp; <a href="https://github.com/poole/hyde/" target="_blank">Hyde</a>.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Goal-Oriented Dialog Generation with Few-Shot Training & Knowledge Transfer</h1>
  <span class="post-date">03 Jun 2020</span>
  <p>This article will help you develop an intuition-based understanding of Goal-Oriented Dialogue Generation in dialogue systems, with Few-Shot training and Knowledge Transfer Networks.</p>

<p>To be specific, we’ll learn about an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialogue model, for interpretable response generation using a minimal amount of data that is not annotated.</p>

<h1 id="why-few-shot-learning">Why few-shot learning?</h1>

<p>Deep Neural Networks have proved to be successful in data-intensive applications. Usually, as shown via conventional practice, a network having numerous parameters has a greater capacity to map the data, and more training data provides the network to have a better generalization.</p>

<p>Such neural networks, in lack of sufficient data, tend to struggle in fixing the weights and biases for the neurons. In contrast, the human brain having much more complex network architecture does not face any difficulty in adapting to new domains. Instead, it excels in learning new concepts with limited data.</p>

<p>Few-shot learning has, therefore, been proposed to close the performance gap between a machine learner and a human learner. In the canonical setting of few-shot learning, there is a known training set and unseen testing set with disjoint categories.</p>

<p>This unique setting of few-shot learning poses an unprecedented challenge in efficiently utilizing the prior information in the training set, which corresponds to the known information or historical information of the human learner.</p>

<p>Hence, NLP researchers leverage the knowledge learned by the main model to improve the performance measure of the architecture on target data.</p>

<h1 id="a-look-inside-few-shot-dialogue-generation">A Look Inside Few-Shot Dialogue Generation</h1>

<center>
<figure>
<img src="/assets/images/fsdg.png" alt="fsdg" style="width: 110%;" />
<figcaption style="font-size: 10pt;">The architecture of Task-Oriented Multi-turn Dialog System with Reinforcement Learning.</figcaption>
</figure>
</center>

<p>Dialogue generation aims at generating human-like responses given a human-to-human dialogue history. In a conventional task-oriented dialogue system, a response (user’s raw utterance) is taken, via either spoken language understanding (SLU) module or a natural language understanding (NLU) module, and converted to a semantic frame of dialogue acts.</p>

<p>These dialogue acts are sent to a dialogue manager which produces the system’s next action in a semantic frame in accordance with a policy using the dialogue state tracker. Then, through natural language generation (NLG), if chosen by the policy to respond, the semantic frame is transformed into human understandable utterances.</p>

<p>The training of deep learning-based dialogue systems is hugely dependent on large amounts of data, which questions the ability of the system to perform in a real-world environment with limited data.</p>

<p>Hence, ‘few-shot learning’ approaches to data-efficient dialogue system training is introduced for a new domain using a latent dialogue act annotation learned in an unsupervised format from a larger multi-domain data source as proposed by <a href="https://arxiv.org/abs/1804.08069">Zhao et al. (2018)</a> (referred to as Latent Action Encoder-Decoder Model).</p>

<h1 id="latent-action-encoder-decoder">Latent Action Encoder-Decoder</h1>

<p>The development of an unsupervised neural recognition model that can discover interpretable meaning representations of utterances as a set of discrete latent variables can improve the effectiveness of a dialogue system. This is achieved with a better interpretation of the system-intentions and modelization of the high-level decision-making policy that enables useful generalization and data-efficient domain adaptation.</p>

<p>Built upon variational autoencoders (VAEs), DI-VAE and DI-VST discover interpretable semantics via autoencoding and context predicting, respectively. The prime focus lies in learning discrete latent representations instead of dense continuous ones because of their high interpretability.</p>

<h3 id="discrete-information-variational-autoencoder">Discrete Information Variational Autoencoder</h3>

<p>Discrete Information Variational Autoencoder (DI-VAE) has been improved upon the traditional Discrete Variational Autoencoder (VAE) through a modification in their learning objective to entertain the anti-information limitation of evidence lower bound objective (ELBO).</p>

<center>
<figure>
<img src="/assets/images/elbo.png" alt="elbo" style="width: 80%;" />
<figcaption style="font-size: 10pt;">Evidence lower bound objective as an expectation over a dataset.</figcaption>
</figure>
</center>
<p>The term KL Divergence in ELBO tries to reduce the mutual information (\(I\)) between latent variables (\(Z\)) and the input data (\(X\)) which explains the anti-information limitation.</p>

<p>The resolution regarding the ignorance of the latent variables during the training phase (anti-information limitation) is to maximize both the data likelihood and mutual information between latent action and input.</p>

<center>
<figure>
<img src="/assets/images/optelbo.png" alt="optelbo" style="width: 80%;" />
<figcaption style="font-size: 10pt;">Joint optimization of ELBO and Mutual Information solves the anti-information limitation in a standard variational autoencoder.</figcaption>
</figure>
</center>

<p>DI-VAE maximizes the ELBO jointly with the mutual information between input data and latent actions. It minimizes the KL Divergence using Batch Prior Regularization which proves to be advantageous over, the usual, Maximum Mean Discrepancy due to its non-linear nature and fundamental difference with annealing.</p>

<p>DI-VAE infers sentence representations by the reconstruction of the input sequence, and hence considered generative.</p>

<h3 id="discrete-information-variational-skip-thought">Discrete Information Variational Skip Thought</h3>

<p>The skip thought is a powerful sentence representation that captures contextual information. It uses a Recurrent Neural Network to encode a sentence and then using the resulting representation it predicts the previous and next sentences.</p>

<p>The signals through auto encoding are enriched by extending the concept of skip thought to Discrete Information Variational Skip Thought (DI-VST) that learns sentence level distributional semantics. It uses the same recognition network as DI-VAE to output z’s posterior distribution. The learning objective, as shown, is maximized by the minimization of KL Divergence term.</p>

<center>
<figure>
<img src="/assets/images/divst.png" alt="divst" style="width: 80%;" />
<figcaption style="font-size: 10pt;">Learning objective for DI-VST where x_p represents previous sentence and x_n as the next sentence.</figcaption>
</figure>
</center>

<h3 id="integration-of-di-vae-and-di-vst-with-encoder-and-decoders">Integration of DI-VAE and DI-VST with Encoder and Decoders</h3>

<center>
<figure>
<img src="/assets/images/integration.png" alt="integration" style="width: 110%;" />
<figcaption style="font-size: 10pt;">Integration with Encoder-Decoders at training.</figcaption>
</figure>
</center>

<p>The role of Recognition Network \(R\) is to map a sentence to the latent variable \(z\), and the generator \(G\) defines the learning signals that will be used to train \(z\)’s representation.</p>

<p>Notably, the recognition network \(R\) does not depend on the context \(c\). This design encourages \(z\) to capture context-independent semantics. At test time, given a context \(c\), the policy network \(\pi\) and encoder-decoder network \(F\) will work together to generate the next response. In short, \(R\), \(G\), \(F\) and \(\pi\) are the four components of the framework.</p>

<p>With the discrete latent variable learned by the recognition and generator network, a dialogue context encoder network encodes the context into a distributed representation. The decoder, then, generates the responses using samples from posterior. Meanwhile, policy network \(\pi\) is trained to predict the aggregated posterior from dialogue context \(c\) via maximum likelihood training. This model is referred to as Latent Action Encoder-Decoder (LAED).</p>

<p>Preliminary training trains two LAED models, both DI-VAE and DI-VST. Then, at the main training stage, the hierarchical encoders of both models were trained and incorporated with Few Shot Dialogue Generation Model’s decoder to obtain an extraordinary performance.</p>

<h1 id="dialogue-knowledge-transfer-network">Dialogue Knowledge Transfer Network</h1>

<p>Goal-oriented multi-domain dialogue systems, after the n-COVID19 outbreak, are widely adopted by industries to cater to the needs of existing as well as prospective customers. The data-driven dialogue systems are in development to reduce the amount of data needed for training. This will prove to save a significant amount of computational costs for enterprises.</p>

<p>Dialogue Knowledge Transfer Network (DiKTNet) is a generative goal-oriented dialogue model designed for few-shot learning, i.e. training only using a few in-domain dialogues. The key underlying concept of this model is transfer learning. DiKTNet makes use of the latent text representation learned from several sources ranging from large-scale general-purpose textual corpora to similar dialogues in domains different to the target one.</p>

<p>A Hierarchical Encoder-Decoder architecture with attention-based copying model is used for few-shot dialogue generation. The main task of this model is, having been trained on all the available source data, to fine-tune on the target data to be further evaluated on the full set of target-domain dialogues. Knowledge Base Information is represented as token sequences and concatenates it to the dialogue context similarly to CopyNet setup. The copy mechanism’s implementation used for such outstanding performance of DiKTNet is the Pointer-Sentinel Mixture Model.</p>

<p>DiKTNet achieves state-of-the-art results with two-stage training:</p>

<p><em>Pre-training Stage:</em> It involves learning of dialogue action representations to capture the dialogue structure by abstracting away from surface forms. DI-VAE and DI-VST are trained on large sources of dialogue corpora from multiple domains, like, MetaLWOz corpus, in an unsupervised way with the objectives as described above and use their discretized latent codes (for both system and user) respectively in the downstream model at the next stage of training.</p>

<p><em>Transfer Learning Stage:</em> At this stage, training for the target task begins using the few-shot dialogue generation architecture as described above. Instead of direct domain transfer, domain-general dialogue understanding is incorporated from the LAED representation trained on MetaLWOz at the previous stage. LAED captures the background top-down dialogue structure: sequences of dialogue acts in a cooperative conversation, latent dialog act-induced clustering of utterances, and the overall phrase structure of spoken utterances.</p>

<center>
<figure>
<img src="/assets/images/diktnet.png" alt="diktnet" style="width: 110%;" />
<figcaption style="font-size: 10pt;">DiKTNet Transfer Learning Stage (Stage II).</figcaption>
</figure>
</center>

<p>Similar to the Few Shot Dialogue Generation, as described above, the DI-VAE is used for reconstruction of the words, and DI-VST for building the context.</p>

<p>By transferring latent dialogue knowledge from multiple sources of varying generality, a model with superior generalization is obtained for an under-represented domain.</p>

<p>Finally, DiKTNet is HRED augmented with both ELMo encoder and LAED representation and it is the unsupervised discrete sentence representation learning method. It has the flexibility to accommodate itself via any encoder-decoder model and does not require much data to train itself.</p>

<h4 id="refrences">Refrences:</h4>
<ul>
  <li><a href="https://arxiv.org/abs/1910.01302">[1910.01302] Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks</a></li>
  <li><a href="https://arxiv.org/abs/1804.08069">[1804.08069] Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation</a></li>
  <li><a href="https://arxiv.org/abs/1908.05854">[1908.05854] Few-Shot Dialogue Generation Without Annotated Data: A Transfer Learning Approach</a></li>
</ul>

</div>

<!-- <div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2020/08/29/intuitive-nn">
            Intuitive Neural Networks
            <small>29 Aug 2020</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div> -->

    </div>

  </body>
</html>
