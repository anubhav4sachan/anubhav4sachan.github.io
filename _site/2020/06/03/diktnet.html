<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Goal-Oriented Dialog Generation with Few-Shot Training &amp; Knowledge Transfer | Anubhav Sachan</title><meta name="generator" content="Jekyll v3.8.7" /><meta property="og:title" content="Goal-Oriented Dialog Generation with Few-Shot Training &amp; Knowledge Transfer" /><meta name="author" content="Anubhav Sachan" /><meta property="og:locale" content="en_US" /><meta name="description" content="This article will help you develop an intuition-based understanding of Goal-Oriented Dialogue Generation in dialogue systems, with Few-Shot training and Knowledge Transfer Networks." /><meta property="og:description" content="This article will help you develop an intuition-based understanding of Goal-Oriented Dialogue Generation in dialogue systems, with Few-Shot training and Knowledge Transfer Networks." /><link rel="canonical" href="http://localhost:4000/2020/06/03/diktnet" /><meta property="og:url" content="http://localhost:4000/2020/06/03/diktnet" /><meta property="og:site_name" content="Anubhav Sachan" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-06-03T00:00:00+05:30" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Goal-Oriented Dialog Generation with Few-Shot Training &amp; Knowledge Transfer" /><meta name="twitter:site" content="@anubhav4sachan" /><meta name="twitter:creator" content="@Anubhav Sachan" /> <script type="application/ld+json"> {"@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/06/03/diktnet"},"url":"http://localhost:4000/2020/06/03/diktnet","author":{"@type":"Person","name":"Anubhav Sachan"},"headline":"Goal-Oriented Dialog Generation with Few-Shot Training &amp; Knowledge Transfer","dateModified":"2020-06-03T00:00:00+05:30","description":"This article will help you develop an intuition-based understanding of Goal-Oriented Dialogue Generation in dialogue systems, with Few-Shot training and Knowledge Transfer Networks.","datePublished":"2020-06-03T00:00:00+05:30","@context":"https://schema.org"}</script> <script src="/assets/katex/katex.min.js"></script> <script src="/assets/pseudocode/pseudocode.js" type="text/javascript"></script><link rel="stylesheet" href="/assets/katex/katex.min.css"><link rel="stylesheet" href="/assets/pseudocode/pseudocode.css" type="text/css"><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Anubhav Sachan" href="/atom.xml"><link rel="alternate" title="Anubhav Sachan" type="application/json" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /><style type="text/css"> @font-face{font-family:'Inter';font-style:normal;font-weight:100;font-display:swap;src:url("/assets/fonts/Inter-Thin.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Thin.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:100;font-display:swap;src:url("/assets/fonts/Inter-ThinItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ThinItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:200;font-display:swap;src:url("/assets/fonts/Inter-ExtraLight.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraLight.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:200;font-display:swap;src:url("/assets/fonts/Inter-ExtraLightItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraLightItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url("/assets/fonts/Inter-Light.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Light.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:300;font-display:swap;src:url("/assets/fonts/Inter-LightItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-LightItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url("/assets/fonts/Inter-Regular.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Regular.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:400;font-display:swap;src:url("/assets/fonts/Inter-Italic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Italic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url("/assets/fonts/Inter-Medium.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Medium.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:500;font-display:swap;src:url("/assets/fonts/Inter-MediumItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-MediumItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url("/assets/fonts/Inter-SemiBold.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-SemiBold.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:600;font-display:swap;src:url("/assets/fonts/Inter-SemiBoldItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-SemiBoldItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:swap;src:url("/assets/fonts/Inter-Bold.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Bold.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:700;font-display:swap;src:url("/assets/fonts/Inter-BoldItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-BoldItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:swap;src:url("/assets/fonts/Inter-ExtraBold.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraBold.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:800;font-display:swap;src:url("/assets/fonts/Inter-ExtraBoldItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-ExtraBoldItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:swap;src:url("/assets/fonts/Inter-Black.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-Black.woff?3.13") format("woff")}@font-face{font-family:'Inter';font-style:italic;font-weight:900;font-display:swap;src:url("/assets/fonts/Inter-BlackItalic.woff2?3.13") format("woff2"),url("/assets/fonts/Inter-BlackItalic.woff?3.13") format("woff")}@font-face{font-family:'Inter var';font-weight:100 900;font-style:normal;font-named-instance:'Regular';font-display:swap;src:url("/assets/fonts/Inter-roman.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var';font-weight:100 900;font-style:italic;font-named-instance:'Italic';font-display:swap;src:url("/assets/fonts/Inter-italic.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var alt';font-weight:100 900;font-style:normal;font-named-instance:'Regular';font-display:swap;src:url("/assets/fonts/Inter-roman.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var alt';font-weight:100 900;font-style:italic;font-named-instance:'Italic';font-display:swap;src:url("/assets/fonts/Inter-italic.var.woff2?3.13") format("woff2")}@font-face{font-family:'Inter var experimental';font-weight:100 900;font-style:oblique 0deg 10deg;font-display:swap;src:url("/assets/fonts/Inter.var.woff2?3.13") format("woff2")}html{font-family:'Inter', 'Helvetica', sans-serif;font-display:swap}@supports (font-variation-settings: normal){html{font-family:'Inter var', 'Helvetica', sans-serif}}body{font-size:1rem;line-height:1.5;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility}a,a:visited{color:inherit}a:hover,a:visited:hover{color:inherit}h1,h2,h3,h4,h5,nav a{font-weight:500}blockquote{background:#f9f9f9;border-left:5px solid black;font-size:120%;margin:2rem 0;padding:1rem}blockquote p{margin:0}blockquote footer{font-size:80%;margin:1rem 0 0 0}dl dt{margin-bottom:0.5rem}dl dd{font-style:italic;margin-bottom:2rem}code,.highlight{background:#edf2f7;overflow:auto}code{word-break:break-all}code{padding:0.1rem 0.3rem}pre{padding:1em}.date{opacity:0.6}html{box-sizing:border-box;margin:0;padding:0}*,*:before,*:after{box-sizing:inherit}body{background-color:#edf2f7;color:#495057}header,main{margin:0 auto;max-width:50rem}.grid{display:flex;flex-wrap:wrap;display:grid;grid-template-columns:1fr;grid-auto-rows:minmax(10rem, auto);grid-gap:1rem}.card{display:flex;align-items:left;justify-content:left;min-height:10rem;position:relative;margin-left:1rem;margin-right:1rem;flex:1 1 10rem}@supports (display: grid){.card{margin:0}}.card:nth-child(1n){background:#96d0c3}.card:nth-child(2n){background:#ef7d4d}.card:nth-child(3n){background:#fbcfb9}.card:nth-child(4n){background:#fac14e}.card:nth-child(5n+1){background:#ef7d4d}.card:nth-child(6n+1){background:#ef7d4d}.card:nth-child(7n+1){background:#fac14e}.card:nth-child(4n+1){background:#fac14e}.card:nth-child(4n+2){background:#fbcfb9}.card:nth-child(4n+3){background:#ef7d4d}.card:nth-child(4n+4){background:#96d0c3}.card{background:white;-webkit-box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);text-decoration:none;color:inherit}.card .thumb{padding-bottom:60%;background-size:cover;background-position:center center}.card a{text-decoration:none}.card p{font-size:0.9rem}.card h2{font-size:1.3rem;margin:0}.card time{font-size:0.8rem;font-weight:500;text-transform:uppercase;letter-spacing:.05em}.card .meta{padding:2rem;display:flex;flex:1;justify-content:space-between;flex-direction:column}.post{background:#fcfff9;padding:2rem 3rem;-webkit-box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03);box-shadow:0 0.75rem 1.5rem rgba(18,38,63,0.03)}.card{transition:all 0.3s cubic-bezier(0.25, 0.45, 0.45, 0.95)}.card:hover{transform:scale(1.02)}ul,ol{padding:0;list-style-position:inside}table{border-collapse:collapse;text-align:left;width:100%}table tr{border-bottom:1px solid black}table td{padding:0.5rem}img{width:100%;margin:0.5rem 0}nav ul{list-style:none !important;padding:0;text-align:center}nav ul li{display:inline-block}nav a,nav a:visited{margin:0.5rem;text-decoration:none;text-transform:uppercase;color:inherit;font-size:0.8rem;font-weight:500;letter-spacing:.05em}footer{margin:1rem 0;text-align:center}.row{display:flex}.column{flex:33%}</style></head><body><header role="banner"><nav role="navigation"><ul><li><a href="/" ><b>Anubhav Sachan</b> &nbsp; | </a></li><li><a href="/about" >Timeline</a></li><li><a href="/experience" >Experience</a></li><li><a href="/blog" >Blog</a></li><li><a href="/projects" >Projects</a></li><li><a href="/assets/Anubhav_Sachan_CV.pdf">Resume</a></li><li><a href="/tags" >Tags</a></li></ul></nav></header><main id="main" role="main"> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><article role="article" class="post"><div class="post-meta"><h1>Goal-Oriented Dialog Generation with Few-Shot Training & Knowledge Transfer</h1><time class="date" datetime="2020-06-03T00:00:00+05:30">June 3, 2020 &middot; <a href="https://medium.com/saarthi-ai/few-shot-goal-oriented-dialogue-generation-with-knowledge-transfer-networks-51486a36c809" target="_blank">Medium &#x2197;</a></time></div><p>This article will help you develop an intuition-based understanding of Goal-Oriented Dialogue Generation in dialogue systems, with Few-Shot training and Knowledge Transfer Networks.</p><p>To be specific, we’ll learn about an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialogue model, for interpretable response generation using a minimal amount of data that is not annotated.</p><h1 id="why-few-shot-learning">Why few-shot learning?</h1><p>Deep Neural Networks have proved to be successful in data-intensive applications. Usually, as shown via conventional practice, a network having numerous parameters has a greater capacity to map the data, and more training data provides the network to have a better generalization.</p><p>Such neural networks, in lack of sufficient data, tend to struggle in fixing the weights and biases for the neurons. In contrast, the human brain having much more complex network architecture does not face any difficulty in adapting to new domains. Instead, it excels in learning new concepts with limited data.</p><p>Few-shot learning has, therefore, been proposed to close the performance gap between a machine learner and a human learner. In the canonical setting of few-shot learning, there is a known training set and unseen testing set with disjoint categories.</p><p>This unique setting of few-shot learning poses an unprecedented challenge in efficiently utilizing the prior information in the training set, which corresponds to the known information or historical information of the human learner.</p><p>Hence, NLP researchers leverage the knowledge learned by the main model to improve the performance measure of the architecture on target data.</p><h1 id="a-look-inside-few-shot-dialogue-generation">A Look Inside Few-Shot Dialogue Generation</h1><center><figure> <img src="/assets/images/fsdg.png" alt="fsdg" style="width: 110%;" /><figcaption style="font-size: 10pt;">The architecture of Task-Oriented Multi-turn Dialog System with Reinforcement Learning.</figcaption></figure></center><p>Dialogue generation aims at generating human-like responses given a human-to-human dialogue history. In a conventional task-oriented dialogue system, a response (user’s raw utterance) is taken, via either spoken language understanding (SLU) module or a natural language understanding (NLU) module, and converted to a semantic frame of dialogue acts.</p><p>These dialogue acts are sent to a dialogue manager which produces the system’s next action in a semantic frame in accordance with a policy using the dialogue state tracker. Then, through natural language generation (NLG), if chosen by the policy to respond, the semantic frame is transformed into human understandable utterances.</p><p>The training of deep learning-based dialogue systems is hugely dependent on large amounts of data, which questions the ability of the system to perform in a real-world environment with limited data.</p><p>Hence, ‘few-shot learning’ approaches to data-efficient dialogue system training is introduced for a new domain using a latent dialogue act annotation learned in an unsupervised format from a larger multi-domain data source as proposed by <a href="https://arxiv.org/abs/1804.08069">Zhao et al. (2018)</a> (referred to as Latent Action Encoder-Decoder Model).</p><h1 id="latent-action-encoder-decoder">Latent Action Encoder-Decoder</h1><p>The development of an unsupervised neural recognition model that can discover interpretable meaning representations of utterances as a set of discrete latent variables can improve the effectiveness of a dialogue system. This is achieved with a better interpretation of the system-intentions and modelization of the high-level decision-making policy that enables useful generalization and data-efficient domain adaptation.</p><p>Built upon variational autoencoders (VAEs), DI-VAE and DI-VST discover interpretable semantics via autoencoding and context predicting, respectively. The prime focus lies in learning discrete latent representations instead of dense continuous ones because of their high interpretability.</p><h3 id="discrete-information-variational-autoencoder">Discrete Information Variational Autoencoder</h3><p>Discrete Information Variational Autoencoder (DI-VAE) has been improved upon the traditional Discrete Variational Autoencoder (VAE) through a modification in their learning objective to entertain the anti-information limitation of evidence lower bound objective (ELBO).</p><center><figure> <img src="/assets/images/elbo.png" alt="elbo" style="width: 80%;" /><figcaption style="font-size: 10pt;">Evidence lower bound objective as an expectation over a dataset.</figcaption></figure></center><p>The term KL Divergence in ELBO tries to reduce the mutual information (<script type="math/tex">I</script>) between latent variables (<script type="math/tex">Z</script>) and the input data (<script type="math/tex">X</script>) which explains the anti-information limitation.</p><p>The resolution regarding the ignorance of the latent variables during the training phase (anti-information limitation) is to maximize both the data likelihood and mutual information between latent action and input.</p><center><figure> <img src="/assets/images/optelbo.png" alt="optelbo" style="width: 80%;" /><figcaption style="font-size: 10pt;">Joint optimization of ELBO and Mutual Information solves the anti-information limitation in a standard variational autoencoder.</figcaption></figure></center><p>DI-VAE maximizes the ELBO jointly with the mutual information between input data and latent actions. It minimizes the KL Divergence using Batch Prior Regularization which proves to be advantageous over, the usual, Maximum Mean Discrepancy due to its non-linear nature and fundamental difference with annealing.</p><p>DI-VAE infers sentence representations by the reconstruction of the input sequence, and hence considered generative.</p><h3 id="discrete-information-variational-skip-thought">Discrete Information Variational Skip Thought</h3><p>The skip thought is a powerful sentence representation that captures contextual information. It uses a Recurrent Neural Network to encode a sentence and then using the resulting representation it predicts the previous and next sentences.</p><p>The signals through auto encoding are enriched by extending the concept of skip thought to Discrete Information Variational Skip Thought (DI-VST) that learns sentence level distributional semantics. It uses the same recognition network as DI-VAE to output z’s posterior distribution. The learning objective, as shown, is maximized by the minimization of KL Divergence term.</p><center><figure> <img src="/assets/images/divst.png" alt="divst" style="width: 80%;" /><figcaption style="font-size: 10pt;">Learning objective for DI-VST where x_p represents previous sentence and x_n as the next sentence.</figcaption></figure></center><h3 id="integration-of-di-vae-and-di-vst-with-encoder-and-decoders">Integration of DI-VAE and DI-VST with Encoder and Decoders</h3><center><figure> <img src="/assets/images/integration.png" alt="integration" style="width: 110%;" /><figcaption style="font-size: 10pt;">Integration with Encoder-Decoders at training.</figcaption></figure></center><p>The role of Recognition Network <script type="math/tex">R</script> is to map a sentence to the latent variable <script type="math/tex">z</script>, and the generator <script type="math/tex">G</script> defines the learning signals that will be used to train <script type="math/tex">z</script>’s representation.</p><p>Notably, the recognition network <script type="math/tex">R</script> does not depend on the context <script type="math/tex">c</script>. This design encourages <script type="math/tex">z</script> to capture context-independent semantics. At test time, given a context <script type="math/tex">c</script>, the policy network <script type="math/tex">\pi</script> and encoder-decoder network <script type="math/tex">F</script> will work together to generate the next response. In short, <script type="math/tex">R</script>, <script type="math/tex">G</script>, <script type="math/tex">F</script> and <script type="math/tex">\pi</script> are the four components of the framework.</p><p>With the discrete latent variable learned by the recognition and generator network, a dialogue context encoder network encodes the context into a distributed representation. The decoder, then, generates the responses using samples from posterior. Meanwhile, policy network <script type="math/tex">\pi</script> is trained to predict the aggregated posterior from dialogue context <script type="math/tex">c</script> via maximum likelihood training. This model is referred to as Latent Action Encoder-Decoder (LAED).</p><p>Preliminary training trains two LAED models, both DI-VAE and DI-VST. Then, at the main training stage, the hierarchical encoders of both models were trained and incorporated with Few Shot Dialogue Generation Model’s decoder to obtain an extraordinary performance.</p><h1 id="dialogue-knowledge-transfer-network">Dialogue Knowledge Transfer Network</h1><p>Goal-oriented multi-domain dialogue systems, after the n-COVID19 outbreak, are widely adopted by industries to cater to the needs of existing as well as prospective customers. The data-driven dialogue systems are in development to reduce the amount of data needed for training. This will prove to save a significant amount of computational costs for enterprises.</p><p>Dialogue Knowledge Transfer Network (DiKTNet) is a generative goal-oriented dialogue model designed for few-shot learning, i.e. training only using a few in-domain dialogues. The key underlying concept of this model is transfer learning. DiKTNet makes use of the latent text representation learned from several sources ranging from large-scale general-purpose textual corpora to similar dialogues in domains different to the target one.</p><p>A Hierarchical Encoder-Decoder architecture with attention-based copying model is used for few-shot dialogue generation. The main task of this model is, having been trained on all the available source data, to fine-tune on the target data to be further evaluated on the full set of target-domain dialogues. Knowledge Base Information is represented as token sequences and concatenates it to the dialogue context similarly to CopyNet setup. The copy mechanism’s implementation used for such outstanding performance of DiKTNet is the Pointer-Sentinel Mixture Model.</p><p>DiKTNet achieves state-of-the-art results with two-stage training:</p><p><em>Pre-training Stage:</em> It involves learning of dialogue action representations to capture the dialogue structure by abstracting away from surface forms. DI-VAE and DI-VST are trained on large sources of dialogue corpora from multiple domains, like, MetaLWOz corpus, in an unsupervised way with the objectives as described above and use their discretized latent codes (for both system and user) respectively in the downstream model at the next stage of training.</p><p><em>Transfer Learning Stage:</em> At this stage, training for the target task begins using the few-shot dialogue generation architecture as described above. Instead of direct domain transfer, domain-general dialogue understanding is incorporated from the LAED representation trained on MetaLWOz at the previous stage. LAED captures the background top-down dialogue structure: sequences of dialogue acts in a cooperative conversation, latent dialog act-induced clustering of utterances, and the overall phrase structure of spoken utterances.</p><center><figure> <img src="/assets/images/diktnet.png" alt="diktnet" style="width: 110%;" /><figcaption style="font-size: 10pt;">DiKTNet Transfer Learning Stage (Stage II).</figcaption></figure></center><p>Similar to the Few Shot Dialogue Generation, as described above, the DI-VAE is used for reconstruction of the words, and DI-VST for building the context.</p><p>By transferring latent dialogue knowledge from multiple sources of varying generality, a model with superior generalization is obtained for an under-represented domain.</p><p>Finally, DiKTNet is HRED augmented with both ELMo encoder and LAED representation and it is the unsupervised discrete sentence representation learning method. It has the flexibility to accommodate itself via any encoder-decoder model and does not require much data to train itself.</p><h4 id="refrences">Refrences:</h4><ul><li><a href="https://arxiv.org/abs/1910.01302">[1910.01302] Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks</a></li><li><a href="https://arxiv.org/abs/1804.08069">[1804.08069] Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation</a></li><li><a href="https://arxiv.org/abs/1908.05854">[1908.05854] Few-Shot Dialogue Generation Without Annotated Data: A Transfer Learning Approach</a></li></ul><br><p> Tagged: <a href="/tag/encoder-decoder">#encoder-decoder</a>, <a href="/tag/unsupervised">#unsupervised</a>, <a href="/tag/representation-learning">#representation-learning</a>, <a href="/tag/few-shot">#few-shot</a>, <a href="/tag/transfer-learning">#transfer-learning</a>, <a href="/tag/deep-learning">#deep-learning</a>, <a href="/tag/reinforcement-learning">#reinforcement-learning</a>, <a href="/tag/nlp">#nlp</a>, <a href="/tag/neural-networks">#neural-networks</a>.</p><hr style="height:2px; border:none; margin: 2rem 0; background-color:#e7e9ee;"><div id="disqus_thread"></div><script> var disqus_name = "your disqus username"; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_name + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); </script> <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></article></main><footer class="footer" role="contentinfo"> <small> &copy; 2020, <a href="/">Anubhav Sachan</a> <br>Social: <a href = "https://www.github.com/anubhav4sachan/" target="_blank">Github</a>, <a href="https://www.twitter.com/anubhav4sachan/" target="_blank">Twitter</a>, <a href = "https://www.linkedin.com/in/anubhav4sachan/" target="_blank">LinkedIn</a> &amp; <a href="https://www.instagram.com/anubhavenue/" target="_blank">Instagram</a>.<br>Feel free to reach via <a href = "mailto:hi@anubhavsachan.com">mail</a>. </small></footer></body></html>
