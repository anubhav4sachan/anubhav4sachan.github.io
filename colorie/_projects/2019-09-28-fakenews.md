---
layout: project
title: Debunking Fake News by Leveraging Speaker Credibility and BERT Based Model
description: Largely an unsupervised learning problem, since there are very few labelled datasets for supervised learning approach. The unsupervised approach largely takes the problem as an anomaly detection task wherein documents deviating from the general character are labelled as fake, inculcating a general assumption that most of the news are not fake.
summary: A novel intuitive approach to exploit data from multiple sources to segregate news into real and fake using contextual embeddings, sequence models with a credibility score for speaker.
category: Transformers, Google BERT, Fake News, Natural Language Processing
organization: CNLP Lab, NIT Silchar
github: Fake-News-Detector
---

# Brief Overview:

In recent years, it has been noticed that an amalgamation of different forms of data fed into a single predictive model has the potential of enhancing the performance of any machine learning technique. With this motivation, we leverage three different types of data as inputs to our proposed fake news detection model. 

Firstly, we use lexical features extracted from the actual statements of the speakers. In addition to this, the speaker’s profile as the metadata for the model has been used.

Speaker’s profile can have a significant impact on the decision-making process for any news to be fake or real. For example, a conservative might undermine notions like abortion rights while a progressive might exaggerate ideas like the removal of internet censorship. 

Lastly, credit history has been used from the Speaker2Credit dataset to inculcate the credibility of the speaker in the decision-making process to infer news as fake.

We introduce a hybrid model that treats fake news detection as a multi-class classification problem. A sequential model is utilized to encode the speaker’s statements. Speaker profile information is then added and the speaker’s credibility is used as an attention factor to form a hybrid model.

The different architectures used for sequential encoding of the speaker’s statements include Long short-term memory (LSTM) networks, Convolutional Neural Network (CNN), LSTM-CNN, Recurrent-CNN (RCNN), etc.

The one-hot encoded vectors of the speaker’s profile and credit vectors have been incorporated in the model by the means of simple concatenation to the encodings of the statements obtained from the sequential model. We also use a simple attention mechanism to refocus the statement encodings in accordance with the speaker’s credibility.