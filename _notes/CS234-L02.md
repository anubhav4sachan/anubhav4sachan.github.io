---
layout: note
title: Making Good Decisions given a Model of the World
lnumber: 02
uni: Stanford University
number: CS234
ctitle: Reinforcement Learning
version: Winter 2019
ytlink: https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u
website: http://web.stanford.edu/class/cs234/index.html
instructor: Emma Brunskill
ilink: https://cs.stanford.edu/people/ebrun/
---
<hr>

## Acting in a MDP

<u> Recap: </u>

- $$S$$ -> State Space; $$A$$ -> Action Space

- $$s'$$ -> $$s_{t+1}$$; $$s$$ -> $$s_{t}$$;  and $$s \in S$$

- Similarly, $$a'$$ -> $$a_{t+1}$$; $$a$$ -> $$a_{t}$$;  and $$a \in A$$. <br>Note that, action $$a$$ is taken in *current* state $$s$$

- **Model:** mathematical description of the dynamics and rewards of the agent's including the transition probability $$P(s' \mid s, a)$$ (Probability of getting into *new* state $$s'$$ given, *current* state $$s$$ and action taken at this state $$s$$ is $$a$$)

- **Policy:** function $$\pi : S \rightarrow A$$ => $$\pi$$ maps states to actions

- **Value Function:** $$V^{\pi}$$ -> particular policy $$\pi$$ gives cumulative sum of future discounted rewards obtained by the agent. <br><center> $$V^{\pi}(s) = \mathbb{E}_{\pi} [r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2}\hspace{2pt}+\hspace{2pt} ... \mid s_t = s]$$</center>

-  **Markov Property:** Future is independent of past, given present <br> <center>$$P(s_i \mid s_0, ..., s_{i−1}) = P(s_i \mid s_{i−1}), \forall  \hspace{2pt} i = 1, 2, ...$$</center>

 <br><br> 
#### Markov Process (=> Memoryless Stochastic Process)
- Stochastic process satisfying Markov Property

- Two assumptions:
	- Finite State Space
	- Stationary Transition Probability: Stationary => Time independent i.e. transition does not depend on time-step and only on states

- Processes with these assumptions => Markov Chain

- Such a transition matrix **P** is a *Row Stochastic Matrix* (means each element is non-negative (since all are  probabilities $$ \in [0, 1]$$) and sum of each row = 1)

- $$\textbf{s P} = \textbf{s'}$$ <br> $$1 \times 6 \cdot 6 \times 6 = 1 \times 6$$

 <br><br> 
#### Markov Reward Process
- tuple of ($$S, \textbf{P}, R, \gamma$$)

- **Reward Function:**

	-	Maps states to rewards (real number) => $$R: S \rightarrow \mathbb{R}$$

	- Each transition $$(s_i \rightarrow s_{i+1})$$ is accompanied with a Reward $$r_i$$

	- $$R(s) = \mathbb{E}[r_i \mid s_i = s]$$

	- *Horizon:* No. of time steps in each episode (can be infinite, else called *finite Markov reward process*)

	- *Return:* Discounted sum of rewards from time t to horizon H <br><center> $$ G_t = \sum^{H-1}_{i = t} \gamma^{i-t} r_i$$ </center>

	- *State Value Function:* Expected return starting from state $$s$$ at time $$t$$ <br><center>
		$$V_t(s) = \mathbb{E} [G_t \mid s_t = s]$$ 
		</center>

	- In  deterministic process, $$G_t$$ and $$V_t(s)$$ are identical. (Stochastic will be different)


- **Discount Factor ($$\gamma$$):**

	- We tend to put more importance in immediate rewards over the rewards obtained at a later time. Hence $$\gamma \in [0, 1)$$ is introduced. <br>If $$\gamma = 1$$, and horizon $$H \rightarrow \infty \Rightarrow V(s) \rightarrow \infty$$.

	- It is fairly accurate to have $$\gamma = 1$$, if $$H$$ is finite. But note that, $$\gamma = 1$$ denotes, all the rewards are equivalent in spite of the fact you reach the final state or not (You may consider a case, when an agent jumps to and fro between two states, infinitely).

	- Geometric progression of $$\gamma$$ is the most efficient and effective way, currently*.

<br><br>
#### Calculation of State Value Function
- **Monte Carlo Method**
	- Generate episode -> Calculate Reward -> Average for each episode
	
	![montecarlo](/assets/nimages/montecarlo.png)

- **Analytical Solution**
	- Works only infinite horizon Markov Reward Process with $$\gamma$$ < 1.

	- $$V(s) = R(s) + \gamma \cdot \sum_{s' \in S} P(s' \mid S) V(s')$$

	- Vectorized Notation: <br> <center> $$V = R + \gamma \textbf{P}V$$</center> <br> $$\textbf{P}$$ is Probability Transition Matrix equivalent to $$P(s' \mid S)$$ 

	- Above equation can be rearranged to <br><center>$$(\textbf{I} - \gamma \textbf{P})V = R$$</center><br>which has an analytical solution <center>$$V = (\textbf{I} - \gamma \textbf{P})^{-1} R$$</center>

	- In above solution, note that, $$\gamma < 1$$, and $$\textbf{P}$$ is row stochastic, $$(\textbf{I} - \gamma \textbf{P})$$ is non-singular => it can be inverted. Thus, $$V = (\textbf{I} - \gamma \textbf{P})^{-1} R$$ always has a solution and the solution is unique.

- **Iterative Method**
	- Finite Horizon => <u>*Dynamic Programming*</u>
		- $$V_t(s) = R(s) + \gamma \cdot \sum_{s' \in S} P(s' \mid S) V_{t+1}(s') \hspace{8pt} \forall \hspace{3pt} t = 0, 1, ..., H - 1.$$

	![algo2-dp](/assets/nimages/dpalgo.png)
	
	- Infinite Horizon => <u>*Iterative*</u>

	![iterative-sol](/assets/nimages/iterative.png)
	
- <u><i>Note:</i></u>
	- The computational cost for Algorithm 1 is $$O(\mid S \mid ^3)$$ where as for Algorithms 2 & 3 it has been improved to $$O(\mid S \mid ^2)$$. All of them converge to the correct solution as described by Theorem 3.1 in Original Lecture Notes.

 <br><br> 
#### Markov Decision Process
-	tuple of ($$S, A, \textbf{P}, R, \gamma$$)

-	Major difference from MRP, is the inclusion of Action space $$A$$:

	-	Now, the transition probability will be defined with the action $$P(s_i = s' \mid s_{i-1} = s, a_{i-1} = a)$$

	-	Similarly, reward is defined as <br><center>$$R(s, a) = \mathbb{E}[r_i \mid s_{i} = s, a_{i} = a]$$</center>
	
-	**MDP Policy**: $$\pi (a \mid s) = P(a_t = a \mid s_t = s)$$ can be deterministic or stochastic. <br> For *generality*: Consider it as _**conditional distribution**_.

-	MDP + $$\pi(a \mid s)$$ = Markov Reward Process (Think about it, if can't find answer, look the printed notes)
	- => MDP can be evaluated in a similar fashion to MRPs.

<br><br>
#### Bellman Backup Operator(s)
We know that, if there is a MDP $$M = (S, A, \textbf{P}, R, \gamma)$$ and a (deterministic/stochastic) policy $$\pi (s|a)$$, it can be considered equivalent to MRP $$M' = (S, \textbf{P}, R, \gamma)$$.

=> The value function of the policy $$\pi$$ evaluated on $$M$$, denoted by $$V^\pi$$, is actually same as value function evaluated on $$M'$$. $$V^\pi$$ lives in the finite dimensional Banach Space ($$\mathbb{R}^{\mid S \mid}$$). 

_**Trivia:**_ Banach Space $$\mathbb{R}^{\mid S \mid}$$ is a complete vector space with norm $$\Vert .\Vert$$. Normed Vector Space is basically a space where norm is defined ($$x$$ -> $$\Vert x \Vert$$) or intuitively, the notion of "length" is captured. 

Then, for an element $$U \in \mathbb{R}^{\mid S \mid}$$, _**Bellman expectation backup operator**_ is defined as <br><center>
$$(B^\pi(s)) = R^\pi(s) + \gamma \sum_{s' \in S} P^\pi(s'|s) \cdot U(s'); \forall s\in S$$</center>

This equation is also termed as _Bellman Expectation Equation_.

Similarly, for an element $$U \in \mathbb{R}^{\mid S \mid}$$, _**Bellman optimality backup operator**_ is defined as <br><center>
$$(B^\pi(s)) = \max_{a \in A} \Big [ R^\pi(s) + \gamma \sum_{s' \in S} P^\pi(s'|s) \cdot U(s')\Big]; \forall s \in S $$</center>

This equation is also termed as _Bellman Optimality Equation_.

The $$\max$$ operation in Optimality Equation brings the non-linearity in $$U$$, due to which, there is no closed form solution as in the case of Expectation Equation. Ref: [StackExchange](https://stats.stackexchange.com/questions/324339/confusion-around-bellman-update-operator)


<br><br>
#### MDP Control
-	$$\pi^*(s) = \arg \max_{\pi} V^{\pi}(s)$$

-	There exists a unique optimal value function.

-	Optimal Policy (not unique, like optimal value function) for a MDP in an infinite horizon problem is deterministic.

<br><br>
#### Policy Iteration & Value Iteration
Taken from [Stack Overflow](https://stackoverflow.com/questions/37370015/what-is-the-difference-between-value-iteration-and-policy-iteration)

- **Policy iteration** includes: **policy evaluation + policy improvement**, and the two are repeated iteratively until policy converges.

- **Value iteration** includes: **finding optimal value function** + one p**olicy extraction**. There is no repeat of the two because once the value function is optimal, then the policy out of it should also be optimal (i.e. converged).

- **Finding optimal value function** can also be seen as a combination of policy improvement (due to max) and truncated policy evaluation (the reassignment of $$V(s)$$ after just one sweep of all states regardless of convergence).

- The algorithms for **policy evaluation** and **finding optimal value function** are ***highly similar*** except for a max operation.

- Similarly, the key step to policy improvement and policy extraction are identical except the former involves a stability check.


---